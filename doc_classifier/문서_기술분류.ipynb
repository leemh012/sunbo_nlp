{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선보엔젤 파트너스 투자 정보 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma,Okt\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import NounLMatchTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import namedtuple\n",
    "import multiprocessing\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading tokenized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_set = pd.read_excel('C:/Users/hangy/Desktop/topics/data/doc_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set['token'] = doc_set['token'].apply(lambda x: x.replace(\"['\",\"\").replace(\"']\",\"\").split(\"', '\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_small = pd.read_excel('C:/Users/hangy/Desktop/topics/data/new_small_class.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dic = {}\n",
    "big = list(set(doc_set['new_class']))\n",
    "for i in range(len(big)):\n",
    "    temp = big[i]\n",
    "    s_temp = list (set(doc_set[doc_set['new_class']==temp].new_small_class))\n",
    "    category_dic[temp]=s_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun corpus\n",
    "def noun_corpus(sents):\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
    "    noun_extractor.train(sents)\n",
    "    nouns = noun_extractor.extract()\n",
    "\n",
    "    noun_scores = {noun:score[0] for noun, score in nouns.items() if len(noun) > 1}\n",
    "    tokenizer = NounLMatchTokenizer(noun_scores)\n",
    "    corpus = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    return corpus\n",
    "\n",
    "ko_stopwords = pd.read_csv('C:/Users/hangy/Desktop/topics/data/korean_stopwords.txt',encoding='utf-8-sig', engine='python')\n",
    "stops = list(ko_stopwords['stopwords'])\n",
    "\n",
    "# stopwords 제거\n",
    "def stopwords (stops,corpus):\n",
    "    docs=[]\n",
    "    for i in range(len(corpus)):\n",
    "        words=[]\n",
    "        for w in corpus[i]:\n",
    "            if (not w in stops) & (len(w)>1):\n",
    "                words.append(w)\n",
    "        docs.append(words)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7138 3060\n"
     ]
    }
   ],
   "source": [
    "label_type = 'new_small_class'\n",
    "df_train, df_test = train_test_split(doc_set, test_size=0.3,random_state=42)\n",
    "print(len(df_train),len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument(d, c) for d, c in df_train[['token', 'new_small_class']].values]\n",
    "tagged_test_docs = [TaggedDocument(d, c) for d, c in df_test[['token', 'new_small_class']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer = Doc2Vec(\n",
    "    dm=0,            # PV-DBOW / default 1\n",
    "    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n",
    "    window=8,        # distance between the predicted word and context words\n",
    "    size=300,        # vector size\n",
    "    alpha=0.025,     # learning-rate\n",
    "    seed=1234,\n",
    "    min_count=5,    # ignore with freq lower\n",
    "    min_alpha=0.025, # min learning-rate\n",
    "    workers=4,   # multi cpu\n",
    "    hs = 1,          # hierarchical softmax / default 0\n",
    "    negative = 10,   # negative sampling / default 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-19 16:40:12,772 : INFO : collecting all words and their counts\n",
      "2019-10-19 16:40:12,774 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-10-19 16:40:14,255 : INFO : collected 46018 word types and 430 unique tags from a corpus of 7138 examples and 2238696 words\n",
      "2019-10-19 16:40:14,256 : INFO : Loading a fresh vocabulary\n",
      "2019-10-19 16:40:14,784 : INFO : effective_min_count=5 retains 25164 unique words (54% of original 46018, drops 20854)\n",
      "2019-10-19 16:40:14,785 : INFO : effective_min_count=5 leaves 2188208 word corpus (97% of original 2238696, drops 50488)\n",
      "2019-10-19 16:40:14,974 : INFO : deleting the raw counts dictionary of 46018 items\n",
      "2019-10-19 16:40:14,977 : INFO : sample=0.001 downsamples 11 most-common words\n",
      "2019-10-19 16:40:14,979 : INFO : downsampling leaves estimated 2184149 word corpus (99.8% of prior 2188208)\n",
      "2019-10-19 16:40:15,030 : INFO : constructing a huffman tree from 25164 words\n",
      "2019-10-19 16:40:15,950 : INFO : built huffman tree with maximum node depth 19\n",
      "2019-10-19 16:40:16,056 : INFO : estimated required memory for 25164 words and 300 dimensions: 108807200 bytes\n",
      "2019-10-19 16:40:16,058 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d300,n10,hs,w8,mc5,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "print(str(doc_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2019-03-28 13:44:15,382 : INFO : training model with 4 workers on 27512 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-03-28 13:44:17,729 : INFO : EPOCH 1 - PROGRESS: at 0.32% examples, 3384 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:20,362 : INFO : EPOCH 1 - PROGRESS: at 1.89% examples, 9512 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:23,259 : INFO : EPOCH 1 - PROGRESS: at 2.97% examples, 10855 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:44:25,796 : INFO : EPOCH 1 - PROGRESS: at 4.63% examples, 12015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:28,427 : INFO : EPOCH 1 - PROGRESS: at 6.26% examples, 12614 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:29,932 : INFO : EPOCH 1 - PROGRESS: at 7.77% examples, 13724 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:32,132 : INFO : EPOCH 1 - PROGRESS: at 9.20% examples, 14261 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:34,596 : INFO : EPOCH 1 - PROGRESS: at 10.86% examples, 14472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:35,602 : INFO : EPOCH 1 - PROGRESS: at 11.28% examples, 14241 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:37,634 : INFO : EPOCH 1 - PROGRESS: at 12.29% examples, 14247 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:39,589 : INFO : EPOCH 1 - PROGRESS: at 12.70% examples, 13498 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:41,652 : INFO : EPOCH 1 - PROGRESS: at 13.99% examples, 13564 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:43,228 : INFO : EPOCH 1 - PROGRESS: at 14.37% examples, 13148 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:44:45,181 : INFO : EPOCH 1 - PROGRESS: at 15.53% examples, 13263 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:46,660 : INFO : EPOCH 1 - PROGRESS: at 15.97% examples, 12948 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:48,205 : INFO : EPOCH 1 - PROGRESS: at 17.12% examples, 13206 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:50,163 : INFO : EPOCH 1 - PROGRESS: at 17.69% examples, 12962 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:51,315 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 13097 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:53,407 : INFO : EPOCH 1 - PROGRESS: at 19.16% examples, 12892 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:54,788 : INFO : EPOCH 1 - PROGRESS: at 19.93% examples, 12937 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:44:57,685 : INFO : EPOCH 1 - PROGRESS: at 20.42% examples, 12513 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:00,961 : INFO : EPOCH 1 - PROGRESS: at 21.49% examples, 12356 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:02,308 : INFO : EPOCH 1 - PROGRESS: at 22.70% examples, 12623 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:45:04,387 : INFO : EPOCH 1 - PROGRESS: at 23.13% examples, 12288 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:05,582 : INFO : EPOCH 1 - PROGRESS: at 24.15% examples, 12584 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:08,098 : INFO : EPOCH 1 - PROGRESS: at 24.55% examples, 12172 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:09,195 : INFO : EPOCH 1 - PROGRESS: at 25.85% examples, 12466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:11,099 : INFO : EPOCH 1 - PROGRESS: at 26.30% examples, 12218 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:13,977 : INFO : EPOCH 1 - PROGRESS: at 27.95% examples, 12279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:16,149 : INFO : EPOCH 1 - PROGRESS: at 29.42% examples, 12438 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:45:18,878 : INFO : EPOCH 1 - PROGRESS: at 30.85% examples, 12517 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:45:21,728 : INFO : EPOCH 1 - PROGRESS: at 32.33% examples, 12569 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:24,819 : INFO : EPOCH 1 - PROGRESS: at 33.84% examples, 12571 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:28,191 : INFO : EPOCH 1 - PROGRESS: at 35.38% examples, 12530 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:31,241 : INFO : EPOCH 1 - PROGRESS: at 36.85% examples, 12538 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:33,918 : INFO : EPOCH 1 - PROGRESS: at 38.25% examples, 12596 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:36,785 : INFO : EPOCH 1 - PROGRESS: at 39.71% examples, 12632 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:45:39,546 : INFO : EPOCH 1 - PROGRESS: at 41.33% examples, 12684 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:42,981 : INFO : EPOCH 1 - PROGRESS: at 43.02% examples, 12639 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:46,558 : INFO : EPOCH 1 - PROGRESS: at 44.63% examples, 12571 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:49,804 : INFO : EPOCH 1 - PROGRESS: at 46.18% examples, 12556 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:53,049 : INFO : EPOCH 1 - PROGRESS: at 47.83% examples, 12545 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:56,164 : INFO : EPOCH 1 - PROGRESS: at 49.20% examples, 12546 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:45:58,852 : INFO : EPOCH 1 - PROGRESS: at 50.85% examples, 12599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:01,765 : INFO : EPOCH 1 - PROGRESS: at 52.53% examples, 12626 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:04,444 : INFO : EPOCH 1 - PROGRESS: at 53.98% examples, 12678 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:08,176 : INFO : EPOCH 1 - PROGRESS: at 55.60% examples, 12604 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:12,210 : INFO : EPOCH 1 - PROGRESS: at 57.24% examples, 12505 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:15,723 : INFO : EPOCH 1 - PROGRESS: at 58.82% examples, 12468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:19,465 : INFO : EPOCH 1 - PROGRESS: at 60.49% examples, 12407 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:23,169 : INFO : EPOCH 1 - PROGRESS: at 61.93% examples, 12353 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:46:26,573 : INFO : EPOCH 1 - PROGRESS: at 63.45% examples, 12331 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:30,206 : INFO : EPOCH 1 - PROGRESS: at 65.06% examples, 12290 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:33,872 : INFO : EPOCH 1 - PROGRESS: at 66.78% examples, 12249 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:37,254 : INFO : EPOCH 1 - PROGRESS: at 68.35% examples, 12233 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:40,347 : INFO : EPOCH 1 - PROGRESS: at 70.06% examples, 12243 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:42,613 : INFO : EPOCH 1 - PROGRESS: at 71.53% examples, 12295 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:44,257 : INFO : EPOCH 1 - PROGRESS: at 72.36% examples, 12290 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:46,425 : INFO : EPOCH 1 - PROGRESS: at 72.97% examples, 12243 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:47,890 : INFO : EPOCH 1 - PROGRESS: at 73.82% examples, 12255 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:50,267 : INFO : EPOCH 1 - PROGRESS: at 74.62% examples, 12194 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:51,913 : INFO : EPOCH 1 - PROGRESS: at 75.38% examples, 12191 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:53,361 : INFO : EPOCH 1 - PROGRESS: at 76.16% examples, 12202 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:54,372 : INFO : EPOCH 1 - PROGRESS: at 76.61% examples, 12185 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:55,426 : INFO : EPOCH 1 - PROGRESS: at 77.39% examples, 12227 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:46:57,242 : INFO : EPOCH 1 - PROGRESS: at 77.82% examples, 12151 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:46:58,857 : INFO : EPOCH 1 - PROGRESS: at 78.47% examples, 12123 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:47:00,672 : INFO : EPOCH 1 - PROGRESS: at 79.43% examples, 12165 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:01,694 : INFO : EPOCH 1 - PROGRESS: at 80.47% examples, 12268 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:04,265 : INFO : EPOCH 1 - PROGRESS: at 80.85% examples, 12138 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:47:05,752 : INFO : EPOCH 1 - PROGRESS: at 81.28% examples, 12090 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:08,573 : INFO : EPOCH 1 - PROGRESS: at 82.37% examples, 12061 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:10,361 : INFO : EPOCH 1 - PROGRESS: at 82.77% examples, 11995 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:12,639 : INFO : EPOCH 1 - PROGRESS: at 83.94% examples, 12005 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:14,071 : INFO : EPOCH 1 - PROGRESS: at 84.33% examples, 11965 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:15,520 : INFO : EPOCH 1 - PROGRESS: at 85.32% examples, 12029 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:17,238 : INFO : EPOCH 1 - PROGRESS: at 85.69% examples, 11970 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:19,260 : INFO : EPOCH 1 - PROGRESS: at 86.93% examples, 11997 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:21,001 : INFO : EPOCH 1 - PROGRESS: at 87.30% examples, 11937 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:22,711 : INFO : EPOCH 1 - PROGRESS: at 88.30% examples, 11983 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:47:24,500 : INFO : EPOCH 1 - PROGRESS: at 88.74% examples, 11921 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:26,292 : INFO : EPOCH 1 - PROGRESS: at 89.92% examples, 11963 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:28,037 : INFO : EPOCH 1 - PROGRESS: at 90.30% examples, 11906 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:29,741 : INFO : EPOCH 1 - PROGRESS: at 91.46% examples, 11951 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:31,471 : INFO : EPOCH 1 - PROGRESS: at 91.87% examples, 11896 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:34,469 : INFO : EPOCH 1 - PROGRESS: at 93.13% examples, 11899 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:37,538 : INFO : EPOCH 1 - PROGRESS: at 94.60% examples, 11914 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:47:41,043 : INFO : EPOCH 1 - PROGRESS: at 96.02% examples, 11901 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:47:42,395 : INFO : EPOCH 1 - PROGRESS: at 97.17% examples, 11964 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:44,790 : INFO : EPOCH 1 - PROGRESS: at 97.57% examples, 11873 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:47,410 : INFO : EPOCH 1 - PROGRESS: at 99.02% examples, 11895 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 13:47:47,412 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 13:47:47,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 13:47:47,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 13:47:47,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 13:47:47,874 : INFO : EPOCH - 1 : training on 2563578 raw words (2551202 effective words) took 212.5s, 12007 effective words/s\n",
      "2019-03-28 13:47:50,405 : INFO : EPOCH 2 - PROGRESS: at 0.32% examples, 3131 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:53,419 : INFO : EPOCH 2 - PROGRESS: at 1.84% examples, 8361 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:56,281 : INFO : EPOCH 2 - PROGRESS: at 2.97% examples, 10170 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:47:59,323 : INFO : EPOCH 2 - PROGRESS: at 4.63% examples, 10927 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:02,239 : INFO : EPOCH 2 - PROGRESS: at 6.26% examples, 11454 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:04,086 : INFO : EPOCH 2 - PROGRESS: at 7.77% examples, 12316 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:05,275 : INFO : EPOCH 2 - PROGRESS: at 8.17% examples, 12036 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:07,507 : INFO : EPOCH 2 - PROGRESS: at 9.20% examples, 12167 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:08,798 : INFO : EPOCH 2 - PROGRESS: at 9.61% examples, 11888 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:10,651 : INFO : EPOCH 2 - PROGRESS: at 10.86% examples, 12209 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:11,956 : INFO : EPOCH 2 - PROGRESS: at 11.28% examples, 11957 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:13,716 : INFO : EPOCH 2 - PROGRESS: at 12.29% examples, 12268 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:14,859 : INFO : EPOCH 2 - PROGRESS: at 12.70% examples, 12109 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:16,484 : INFO : EPOCH 2 - PROGRESS: at 13.99% examples, 12455 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:17,727 : INFO : EPOCH 2 - PROGRESS: at 14.37% examples, 12265 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:19,322 : INFO : EPOCH 2 - PROGRESS: at 15.53% examples, 12568 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:20,331 : INFO : EPOCH 2 - PROGRESS: at 15.97% examples, 12479 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:21,657 : INFO : EPOCH 2 - PROGRESS: at 17.12% examples, 12833 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:23,184 : INFO : EPOCH 2 - PROGRESS: at 17.69% examples, 12769 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:24,367 : INFO : EPOCH 2 - PROGRESS: at 18.36% examples, 12897 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:25,749 : INFO : EPOCH 2 - PROGRESS: at 19.16% examples, 12945 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:26,916 : INFO : EPOCH 2 - PROGRESS: at 19.93% examples, 13059 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:48:28,456 : INFO : EPOCH 2 - PROGRESS: at 20.42% examples, 13045 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:30,847 : INFO : EPOCH 2 - PROGRESS: at 21.49% examples, 13107 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:32,164 : INFO : EPOCH 2 - PROGRESS: at 22.70% examples, 13376 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:33,805 : INFO : EPOCH 2 - PROGRESS: at 23.13% examples, 13111 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:35,869 : INFO : EPOCH 2 - PROGRESS: at 24.15% examples, 13163 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:38,921 : INFO : EPOCH 2 - PROGRESS: at 24.55% examples, 12571 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:40,227 : INFO : EPOCH 2 - PROGRESS: at 25.85% examples, 12814 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:42,206 : INFO : EPOCH 2 - PROGRESS: at 26.24% examples, 12526 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:43,666 : INFO : EPOCH 2 - PROGRESS: at 27.53% examples, 12725 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:45,780 : INFO : EPOCH 2 - PROGRESS: at 27.95% examples, 12426 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:47,296 : INFO : EPOCH 2 - PROGRESS: at 29.15% examples, 12601 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:48,708 : INFO : EPOCH 2 - PROGRESS: at 29.42% examples, 12425 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:49,963 : INFO : EPOCH 2 - PROGRESS: at 30.45% examples, 12641 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:52,089 : INFO : EPOCH 2 - PROGRESS: at 30.85% examples, 12377 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:53,141 : INFO : EPOCH 2 - PROGRESS: at 31.88% examples, 12625 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:55,650 : INFO : EPOCH 2 - PROGRESS: at 32.28% examples, 12302 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:48:57,316 : INFO : EPOCH 2 - PROGRESS: at 33.48% examples, 12433 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:00,081 : INFO : EPOCH 2 - PROGRESS: at 33.84% examples, 12089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:01,172 : INFO : EPOCH 2 - PROGRESS: at 34.61% examples, 12178 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:03,645 : INFO : EPOCH 2 - PROGRESS: at 35.38% examples, 12039 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:49:06,487 : INFO : EPOCH 2 - PROGRESS: at 36.85% examples, 12103 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:49:07,890 : INFO : EPOCH 2 - PROGRESS: at 37.90% examples, 12243 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:10,104 : INFO : EPOCH 2 - PROGRESS: at 38.25% examples, 12029 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:11,179 : INFO : EPOCH 2 - PROGRESS: at 39.31% examples, 12225 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:13,886 : INFO : EPOCH 2 - PROGRESS: at 39.71% examples, 11954 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:14,898 : INFO : EPOCH 2 - PROGRESS: at 40.93% examples, 12155 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:49:17,546 : INFO : EPOCH 2 - PROGRESS: at 41.33% examples, 11904 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:21,089 : INFO : EPOCH 2 - PROGRESS: at 43.02% examples, 11877 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:24,467 : INFO : EPOCH 2 - PROGRESS: at 44.63% examples, 11866 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:28,860 : INFO : EPOCH 2 - PROGRESS: at 46.33% examples, 11740 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:49:30,048 : INFO : EPOCH 2 - PROGRESS: at 47.41% examples, 11893 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:49:33,475 : INFO : EPOCH 2 - PROGRESS: at 47.83% examples, 11602 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:36,320 : INFO : EPOCH 2 - PROGRESS: at 49.20% examples, 11659 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:39,167 : INFO : EPOCH 2 - PROGRESS: at 50.85% examples, 11713 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:42,471 : INFO : EPOCH 2 - PROGRESS: at 52.53% examples, 11721 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:45,171 : INFO : EPOCH 2 - PROGRESS: at 53.98% examples, 11787 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:47,807 : INFO : EPOCH 2 - PROGRESS: at 55.60% examples, 11853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:50,621 : INFO : EPOCH 2 - PROGRESS: at 57.24% examples, 11901 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:49:53,293 : INFO : EPOCH 2 - PROGRESS: at 58.82% examples, 11963 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:55,949 : INFO : EPOCH 2 - PROGRESS: at 60.49% examples, 12020 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:49:58,938 : INFO : EPOCH 2 - PROGRESS: at 61.93% examples, 12044 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:50:01,813 : INFO : EPOCH 2 - PROGRESS: at 63.45% examples, 12077 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:05,356 : INFO : EPOCH 2 - PROGRESS: at 65.06% examples, 12052 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:08,669 : INFO : EPOCH 2 - PROGRESS: at 66.78% examples, 12049 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:12,019 : INFO : EPOCH 2 - PROGRESS: at 68.35% examples, 12040 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:15,222 : INFO : EPOCH 2 - PROGRESS: at 70.06% examples, 12045 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:17,042 : INFO : EPOCH 2 - PROGRESS: at 71.53% examples, 12136 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:18,309 : INFO : EPOCH 2 - PROGRESS: at 72.36% examples, 12163 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:20,264 : INFO : EPOCH 2 - PROGRESS: at 72.97% examples, 12135 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:21,277 : INFO : EPOCH 2 - PROGRESS: at 73.40% examples, 12119 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:23,168 : INFO : EPOCH 2 - PROGRESS: at 74.62% examples, 12162 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:50:24,337 : INFO : EPOCH 2 - PROGRESS: at 75.38% examples, 12197 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:50:25,611 : INFO : EPOCH 2 - PROGRESS: at 76.16% examples, 12221 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:27,174 : INFO : EPOCH 2 - PROGRESS: at 77.00% examples, 12222 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:28,439 : INFO : EPOCH 2 - PROGRESS: at 77.82% examples, 12249 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:29,882 : INFO : EPOCH 2 - PROGRESS: at 78.24% examples, 12200 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:30,969 : INFO : EPOCH 2 - PROGRESS: at 78.81% examples, 12209 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:33,683 : INFO : EPOCH 2 - PROGRESS: at 79.43% examples, 12127 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:34,742 : INFO : EPOCH 2 - PROGRESS: at 80.06% examples, 12169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:37,465 : INFO : EPOCH 2 - PROGRESS: at 80.85% examples, 12088 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:38,536 : INFO : EPOCH 2 - PROGRESS: at 81.28% examples, 12069 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:40,477 : INFO : EPOCH 2 - PROGRESS: at 82.37% examples, 12103 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:41,980 : INFO : EPOCH 2 - PROGRESS: at 82.75% examples, 12053 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:44,496 : INFO : EPOCH 2 - PROGRESS: at 83.94% examples, 12049 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:50:45,604 : INFO : EPOCH 2 - PROGRESS: at 84.70% examples, 12084 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:47,003 : INFO : EPOCH 2 - PROGRESS: at 85.32% examples, 12097 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:48,467 : INFO : EPOCH 2 - PROGRESS: at 85.69% examples, 12053 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:50,612 : INFO : EPOCH 2 - PROGRESS: at 86.93% examples, 12072 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:51,619 : INFO : EPOCH 2 - PROGRESS: at 87.71% examples, 12111 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:50:53,324 : INFO : EPOCH 2 - PROGRESS: at 88.30% examples, 12105 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:54,401 : INFO : EPOCH 2 - PROGRESS: at 88.74% examples, 12087 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:57,455 : INFO : EPOCH 2 - PROGRESS: at 89.92% examples, 12047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:50:58,631 : INFO : EPOCH 2 - PROGRESS: at 90.30% examples, 12025 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:00,332 : INFO : EPOCH 2 - PROGRESS: at 91.46% examples, 12070 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:01,623 : INFO : EPOCH 2 - PROGRESS: at 91.85% examples, 12040 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:51:04,332 : INFO : EPOCH 2 - PROGRESS: at 93.13% examples, 12059 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:07,745 : INFO : EPOCH 2 - PROGRESS: at 94.60% examples, 12050 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:09,196 : INFO : EPOCH 2 - PROGRESS: at 95.71% examples, 12109 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:11,754 : INFO : EPOCH 2 - PROGRESS: at 96.02% examples, 12005 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:12,997 : INFO : EPOCH 2 - PROGRESS: at 97.17% examples, 12075 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:15,219 : INFO : EPOCH 2 - PROGRESS: at 97.57% examples, 11991 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:16,229 : INFO : EPOCH 2 - PROGRESS: at 98.34% examples, 12024 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-28 13:51:18,755 : INFO : EPOCH 2 - PROGRESS: at 99.12% examples, 11973 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 13:51:18,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 13:51:18,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 13:51:18,877 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 13:51:19,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 13:51:19,618 : INFO : EPOCH - 2 : training on 2563578 raw words (2551241 effective words) took 211.7s, 12049 effective words/s\n",
      "2019-03-28 13:51:22,117 : INFO : EPOCH 3 - PROGRESS: at 0.32% examples, 3197 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:25,962 : INFO : EPOCH 3 - PROGRESS: at 1.84% examples, 7326 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:29,096 : INFO : EPOCH 3 - PROGRESS: at 2.97% examples, 9037 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:51:32,269 : INFO : EPOCH 3 - PROGRESS: at 4.62% examples, 9887 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:35,239 : INFO : EPOCH 3 - PROGRESS: at 6.26% examples, 10546 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:37,511 : INFO : EPOCH 3 - PROGRESS: at 7.77% examples, 11170 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:38,894 : INFO : EPOCH 3 - PROGRESS: at 8.17% examples, 10875 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:40,540 : INFO : EPOCH 3 - PROGRESS: at 9.20% examples, 11427 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:41,857 : INFO : EPOCH 3 - PROGRESS: at 9.61% examples, 11187 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:44,387 : INFO : EPOCH 3 - PROGRESS: at 10.86% examples, 11235 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:45,696 : INFO : EPOCH 3 - PROGRESS: at 11.28% examples, 11050 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:51:47,465 : INFO : EPOCH 3 - PROGRESS: at 12.29% examples, 11392 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:51:49,119 : INFO : EPOCH 3 - PROGRESS: at 12.70% examples, 11083 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:50,576 : INFO : EPOCH 3 - PROGRESS: at 13.99% examples, 11517 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:52,200 : INFO : EPOCH 3 - PROGRESS: at 14.38% examples, 11240 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:54,046 : INFO : EPOCH 3 - PROGRESS: at 15.53% examples, 11485 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:56,549 : INFO : EPOCH 3 - PROGRESS: at 15.97% examples, 10972 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:51:58,101 : INFO : EPOCH 3 - PROGRESS: at 17.12% examples, 11270 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:51:59,296 : INFO : EPOCH 3 - PROGRESS: at 17.28% examples, 11120 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:01,257 : INFO : EPOCH 3 - PROGRESS: at 17.69% examples, 10832 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:02,489 : INFO : EPOCH 3 - PROGRESS: at 18.36% examples, 10983 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:04,741 : INFO : EPOCH 3 - PROGRESS: at 19.16% examples, 10869 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:06,504 : INFO : EPOCH 3 - PROGRESS: at 20.16% examples, 11085 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:08,181 : INFO : EPOCH 3 - PROGRESS: at 20.42% examples, 10904 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:09,358 : INFO : EPOCH 3 - PROGRESS: at 21.24% examples, 11130 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:11,081 : INFO : EPOCH 3 - PROGRESS: at 21.49% examples, 10948 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:12,838 : INFO : EPOCH 3 - PROGRESS: at 22.70% examples, 11135 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:14,685 : INFO : EPOCH 3 - PROGRESS: at 23.13% examples, 10939 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:16,510 : INFO : EPOCH 3 - PROGRESS: at 24.15% examples, 11108 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:18,859 : INFO : EPOCH 3 - PROGRESS: at 24.55% examples, 10835 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:20,357 : INFO : EPOCH 3 - PROGRESS: at 25.85% examples, 11048 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:22,273 : INFO : EPOCH 3 - PROGRESS: at 26.24% examples, 10864 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:23,889 : INFO : EPOCH 3 - PROGRESS: at 27.53% examples, 11048 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:26,005 : INFO : EPOCH 3 - PROGRESS: at 27.95% examples, 10840 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:27,443 : INFO : EPOCH 3 - PROGRESS: at 29.15% examples, 11042 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:29,108 : INFO : EPOCH 3 - PROGRESS: at 29.42% examples, 10879 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:30,601 : INFO : EPOCH 3 - PROGRESS: at 30.45% examples, 11059 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:32,300 : INFO : EPOCH 3 - PROGRESS: at 30.85% examples, 10937 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:33,680 : INFO : EPOCH 3 - PROGRESS: at 31.88% examples, 11128 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:35,741 : INFO : EPOCH 3 - PROGRESS: at 32.33% examples, 10956 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:37,180 : INFO : EPOCH 3 - PROGRESS: at 33.48% examples, 11134 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:38,906 : INFO : EPOCH 3 - PROGRESS: at 33.85% examples, 11016 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:40,283 : INFO : EPOCH 3 - PROGRESS: at 35.00% examples, 11188 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:42,350 : INFO : EPOCH 3 - PROGRESS: at 35.38% examples, 11029 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:43,459 : INFO : EPOCH 3 - PROGRESS: at 36.43% examples, 11233 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:45,439 : INFO : EPOCH 3 - PROGRESS: at 36.85% examples, 11089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:46,576 : INFO : EPOCH 3 - PROGRESS: at 37.90% examples, 11268 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:48,803 : INFO : EPOCH 3 - PROGRESS: at 38.25% examples, 11093 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:51,920 : INFO : EPOCH 3 - PROGRESS: at 39.71% examples, 11141 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:54,857 : INFO : EPOCH 3 - PROGRESS: at 41.33% examples, 11210 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:52:55,999 : INFO : EPOCH 3 - PROGRESS: at 42.59% examples, 11385 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:52:58,963 : INFO : EPOCH 3 - PROGRESS: at 43.02% examples, 11146 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:00,044 : INFO : EPOCH 3 - PROGRESS: at 43.85% examples, 11219 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:03,403 : INFO : EPOCH 3 - PROGRESS: at 44.63% examples, 11045 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:53:04,555 : INFO : EPOCH 3 - PROGRESS: at 45.92% examples, 11205 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:08,127 : INFO : EPOCH 3 - PROGRESS: at 46.33% examples, 10927 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:53:09,395 : INFO : EPOCH 3 - PROGRESS: at 47.41% examples, 11071 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:11,927 : INFO : EPOCH 3 - PROGRESS: at 47.83% examples, 10909 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:15,531 : INFO : EPOCH 3 - PROGRESS: at 49.28% examples, 10911 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:18,610 : INFO : EPOCH 3 - PROGRESS: at 50.85% examples, 10957 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:21,837 : INFO : EPOCH 3 - PROGRESS: at 52.53% examples, 10991 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:53:25,710 : INFO : EPOCH 3 - PROGRESS: at 53.98% examples, 10966 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:29,213 : INFO : EPOCH 3 - PROGRESS: at 55.60% examples, 10971 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:32,180 : INFO : EPOCH 3 - PROGRESS: at 57.24% examples, 11021 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:35,113 : INFO : EPOCH 3 - PROGRESS: at 58.82% examples, 11074 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:39,479 : INFO : EPOCH 3 - PROGRESS: at 60.49% examples, 11008 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:42,888 : INFO : EPOCH 3 - PROGRESS: at 61.93% examples, 11018 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:46,820 : INFO : EPOCH 3 - PROGRESS: at 63.45% examples, 10990 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:53:50,356 : INFO : EPOCH 3 - PROGRESS: at 65.06% examples, 10993 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:53:54,384 : INFO : EPOCH 3 - PROGRESS: at 66.78% examples, 10961 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:57,933 : INFO : EPOCH 3 - PROGRESS: at 68.35% examples, 10963 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:53:59,050 : INFO : EPOCH 3 - PROGRESS: at 69.64% examples, 11071 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:54:02,296 : INFO : EPOCH 3 - PROGRESS: at 70.06% examples, 10910 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:04,555 : INFO : EPOCH 3 - PROGRESS: at 71.53% examples, 10976 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:54:05,588 : INFO : EPOCH 3 - PROGRESS: at 71.94% examples, 10965 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:06,606 : INFO : EPOCH 3 - PROGRESS: at 72.58% examples, 11017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:08,270 : INFO : EPOCH 3 - PROGRESS: at 72.97% examples, 10965 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:09,327 : INFO : EPOCH 3 - PROGRESS: at 73.40% examples, 10955 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:10,371 : INFO : EPOCH 3 - PROGRESS: at 74.20% examples, 11003 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:12,574 : INFO : EPOCH 3 - PROGRESS: at 74.62% examples, 10920 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:13,690 : INFO : EPOCH 3 - PROGRESS: at 74.99% examples, 10907 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:16,202 : INFO : EPOCH 3 - PROGRESS: at 76.16% examples, 10917 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:17,656 : INFO : EPOCH 3 - PROGRESS: at 76.61% examples, 10882 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:19,530 : INFO : EPOCH 3 - PROGRESS: at 77.82% examples, 10932 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:21,434 : INFO : EPOCH 3 - PROGRESS: at 78.24% examples, 10872 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:25,441 : INFO : EPOCH 3 - PROGRESS: at 79.43% examples, 10821 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:54:28,561 : INFO : EPOCH 3 - PROGRESS: at 80.85% examples, 10850 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:31,211 : INFO : EPOCH 3 - PROGRESS: at 82.37% examples, 10903 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:32,217 : INFO : EPOCH 3 - PROGRESS: at 82.75% examples, 10896 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:54:34,093 : INFO : EPOCH 3 - PROGRESS: at 83.94% examples, 10943 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:36,775 : INFO : EPOCH 3 - PROGRESS: at 85.32% examples, 10991 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:37,955 : INFO : EPOCH 3 - PROGRESS: at 85.74% examples, 10974 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:40,079 : INFO : EPOCH 3 - PROGRESS: at 86.93% examples, 11005 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:41,213 : INFO : EPOCH 3 - PROGRESS: at 87.34% examples, 10991 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:43,495 : INFO : EPOCH 3 - PROGRESS: at 88.30% examples, 11011 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:44,657 : INFO : EPOCH 3 - PROGRESS: at 89.18% examples, 11045 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:47,411 : INFO : EPOCH 3 - PROGRESS: at 89.92% examples, 10991 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:49,160 : INFO : EPOCH 3 - PROGRESS: at 90.30% examples, 10947 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:52,385 : INFO : EPOCH 3 - PROGRESS: at 91.46% examples, 10918 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:53,921 : INFO : EPOCH 3 - PROGRESS: at 91.85% examples, 10885 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:55,454 : INFO : EPOCH 3 - PROGRESS: at 92.91% examples, 10930 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:54:59,458 : INFO : EPOCH 3 - PROGRESS: at 93.13% examples, 10776 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:55:01,555 : INFO : EPOCH 3 - PROGRESS: at 94.43% examples, 10808 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:04,038 : INFO : EPOCH 3 - PROGRESS: at 94.60% examples, 10732 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:06,053 : INFO : EPOCH 3 - PROGRESS: at 95.71% examples, 10766 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:08,786 : INFO : EPOCH 3 - PROGRESS: at 96.02% examples, 10681 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:10,426 : INFO : EPOCH 3 - PROGRESS: at 97.17% examples, 10731 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:13,072 : INFO : EPOCH 3 - PROGRESS: at 97.58% examples, 10650 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:15,297 : INFO : EPOCH 3 - PROGRESS: at 98.72% examples, 10672 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-28 13:55:17,330 : INFO : EPOCH 3 - PROGRESS: at 99.02% examples, 10610 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 13:55:17,332 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 13:55:17,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 13:55:17,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 13:55:18,798 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 10667 words/s, in_qsize 0, out_qsize 1\n",
      "2019-03-28 13:55:18,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 13:55:18,802 : INFO : EPOCH - 3 : training on 2563578 raw words (2551077 effective words) took 239.2s, 10667 effective words/s\n",
      "2019-03-28 13:55:22,753 : INFO : EPOCH 4 - PROGRESS: at 0.32% examples, 2009 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:27,413 : INFO : EPOCH 4 - PROGRESS: at 1.89% examples, 5500 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:30,721 : INFO : EPOCH 4 - PROGRESS: at 2.97% examples, 7176 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:34,203 : INFO : EPOCH 4 - PROGRESS: at 4.63% examples, 8125 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:37,504 : INFO : EPOCH 4 - PROGRESS: at 6.26% examples, 8800 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:39,579 : INFO : EPOCH 4 - PROGRESS: at 7.77% examples, 9612 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:55:40,842 : INFO : EPOCH 4 - PROGRESS: at 8.20% examples, 9502 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:43,264 : INFO : EPOCH 4 - PROGRESS: at 9.20% examples, 9767 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:44,366 : INFO : EPOCH 4 - PROGRESS: at 9.61% examples, 9726 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:55:46,771 : INFO : EPOCH 4 - PROGRESS: at 10.86% examples, 9944 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:49,706 : INFO : EPOCH 4 - PROGRESS: at 12.29% examples, 10260 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:50,865 : INFO : EPOCH 4 - PROGRESS: at 12.71% examples, 10198 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:55:53,138 : INFO : EPOCH 4 - PROGRESS: at 13.99% examples, 10379 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:54,340 : INFO : EPOCH 4 - PROGRESS: at 14.37% examples, 10303 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:56,355 : INFO : EPOCH 4 - PROGRESS: at 15.53% examples, 10525 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:57,512 : INFO : EPOCH 4 - PROGRESS: at 16.39% examples, 10719 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:55:59,100 : INFO : EPOCH 4 - PROGRESS: at 17.12% examples, 10758 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:00,370 : INFO : EPOCH 4 - PROGRESS: at 17.69% examples, 10847 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:02,185 : INFO : EPOCH 4 - PROGRESS: at 18.31% examples, 10844 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:03,382 : INFO : EPOCH 4 - PROGRESS: at 19.16% examples, 10998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:05,647 : INFO : EPOCH 4 - PROGRESS: at 19.78% examples, 10885 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:07,116 : INFO : EPOCH 4 - PROGRESS: at 20.42% examples, 10957 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:08,527 : INFO : EPOCH 4 - PROGRESS: at 21.11% examples, 10987 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:10,547 : INFO : EPOCH 4 - PROGRESS: at 21.49% examples, 10885 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:12,299 : INFO : EPOCH 4 - PROGRESS: at 22.26% examples, 10891 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:14,166 : INFO : EPOCH 4 - PROGRESS: at 23.13% examples, 10877 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:15,417 : INFO : EPOCH 4 - PROGRESS: at 23.73% examples, 10986 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:17,177 : INFO : EPOCH 4 - PROGRESS: at 24.55% examples, 10993 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:18,513 : INFO : EPOCH 4 - PROGRESS: at 25.85% examples, 11236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:20,204 : INFO : EPOCH 4 - PROGRESS: at 26.30% examples, 11088 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:21,247 : INFO : EPOCH 4 - PROGRESS: at 27.09% examples, 11212 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:23,072 : INFO : EPOCH 4 - PROGRESS: at 27.95% examples, 11196 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:24,328 : INFO : EPOCH 4 - PROGRESS: at 29.15% examples, 11427 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:25,952 : INFO : EPOCH 4 - PROGRESS: at 29.52% examples, 11295 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:29,167 : INFO : EPOCH 4 - PROGRESS: at 30.85% examples, 11295 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:32,221 : INFO : EPOCH 4 - PROGRESS: at 32.33% examples, 11359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:33,357 : INFO : EPOCH 4 - PROGRESS: at 33.48% examples, 11581 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:35,685 : INFO : EPOCH 4 - PROGRESS: at 33.85% examples, 11359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:36,802 : INFO : EPOCH 4 - PROGRESS: at 35.00% examples, 11569 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:39,408 : INFO : EPOCH 4 - PROGRESS: at 35.38% examples, 11318 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:42,555 : INFO : EPOCH 4 - PROGRESS: at 36.85% examples, 11357 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:43,755 : INFO : EPOCH 4 - PROGRESS: at 37.90% examples, 11532 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:45,568 : INFO : EPOCH 4 - PROGRESS: at 38.25% examples, 11401 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:46,591 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 11602 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:56:48,519 : INFO : EPOCH 4 - PROGRESS: at 39.71% examples, 11461 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:51,758 : INFO : EPOCH 4 - PROGRESS: at 41.33% examples, 11484 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:53,137 : INFO : EPOCH 4 - PROGRESS: at 42.20% examples, 11526 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:55,272 : INFO : EPOCH 4 - PROGRESS: at 43.02% examples, 11477 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:56:56,916 : INFO : EPOCH 4 - PROGRESS: at 43.85% examples, 11482 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:56:59,998 : INFO : EPOCH 4 - PROGRESS: at 44.63% examples, 11326 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:01,081 : INFO : EPOCH 4 - PROGRESS: at 45.52% examples, 11399 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:04,179 : INFO : EPOCH 4 - PROGRESS: at 46.33% examples, 11251 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:05,687 : INFO : EPOCH 4 - PROGRESS: at 47.41% examples, 11370 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:09,149 : INFO : EPOCH 4 - PROGRESS: at 47.83% examples, 11103 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:13,105 : INFO : EPOCH 4 - PROGRESS: at 49.28% examples, 11063 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:16,448 : INFO : EPOCH 4 - PROGRESS: at 50.85% examples, 11081 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:17,498 : INFO : EPOCH 4 - PROGRESS: at 52.13% examples, 11234 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:19,556 : INFO : EPOCH 4 - PROGRESS: at 52.53% examples, 11124 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:20,649 : INFO : EPOCH 4 - PROGRESS: at 53.66% examples, 11268 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:23,670 : INFO : EPOCH 4 - PROGRESS: at 53.98% examples, 11073 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:24,763 : INFO : EPOCH 4 - PROGRESS: at 54.77% examples, 11132 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:27,987 : INFO : EPOCH 4 - PROGRESS: at 55.60% examples, 11005 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:29,243 : INFO : EPOCH 4 - PROGRESS: at 56.84% examples, 11124 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:32,356 : INFO : EPOCH 4 - PROGRESS: at 57.24% examples, 10938 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:33,434 : INFO : EPOCH 4 - PROGRESS: at 58.43% examples, 11071 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:37,030 : INFO : EPOCH 4 - PROGRESS: at 58.82% examples, 10854 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:38,109 : INFO : EPOCH 4 - PROGRESS: at 60.07% examples, 10982 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:57:41,228 : INFO : EPOCH 4 - PROGRESS: at 60.49% examples, 10809 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:42,595 : INFO : EPOCH 4 - PROGRESS: at 61.53% examples, 10910 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:45,913 : INFO : EPOCH 4 - PROGRESS: at 61.93% examples, 10730 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:47,102 : INFO : EPOCH 4 - PROGRESS: at 63.22% examples, 10843 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:50,035 : INFO : EPOCH 4 - PROGRESS: at 63.58% examples, 10698 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:54,340 : INFO : EPOCH 4 - PROGRESS: at 65.06% examples, 10653 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:57:59,062 : INFO : EPOCH 4 - PROGRESS: at 66.78% examples, 10585 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:03,810 : INFO : EPOCH 4 - PROGRESS: at 68.35% examples, 10518 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:05,142 : INFO : EPOCH 4 - PROGRESS: at 69.64% examples, 10611 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:08,807 : INFO : EPOCH 4 - PROGRESS: at 70.06% examples, 10440 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:11,723 : INFO : EPOCH 4 - PROGRESS: at 71.53% examples, 10469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:13,338 : INFO : EPOCH 4 - PROGRESS: at 72.36% examples, 10483 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:15,552 : INFO : EPOCH 4 - PROGRESS: at 72.97% examples, 10463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:16,615 : INFO : EPOCH 4 - PROGRESS: at 74.20% examples, 10566 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:18,779 : INFO : EPOCH 4 - PROGRESS: at 74.62% examples, 10494 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:19,783 : INFO : EPOCH 4 - PROGRESS: at 75.77% examples, 10599 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:22,227 : INFO : EPOCH 4 - PROGRESS: at 76.16% examples, 10509 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:23,248 : INFO : EPOCH 4 - PROGRESS: at 77.00% examples, 10556 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:25,698 : INFO : EPOCH 4 - PROGRESS: at 77.82% examples, 10523 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:26,915 : INFO : EPOCH 4 - PROGRESS: at 78.98% examples, 10610 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:29,179 : INFO : EPOCH 4 - PROGRESS: at 79.43% examples, 10562 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:30,383 : INFO : EPOCH 4 - PROGRESS: at 80.06% examples, 10599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:32,598 : INFO : EPOCH 4 - PROGRESS: at 80.85% examples, 10578 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:33,959 : INFO : EPOCH 4 - PROGRESS: at 81.70% examples, 10605 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:35,649 : INFO : EPOCH 4 - PROGRESS: at 82.37% examples, 10612 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:37,152 : INFO : EPOCH 4 - PROGRESS: at 82.77% examples, 10581 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:38,440 : INFO : EPOCH 4 - PROGRESS: at 83.56% examples, 10610 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:39,793 : INFO : EPOCH 4 - PROGRESS: at 83.94% examples, 10588 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:40,850 : INFO : EPOCH 4 - PROGRESS: at 84.33% examples, 10581 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:42,161 : INFO : EPOCH 4 - PROGRESS: at 85.10% examples, 10609 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:44,331 : INFO : EPOCH 4 - PROGRESS: at 85.69% examples, 10591 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:45,734 : INFO : EPOCH 4 - PROGRESS: at 86.55% examples, 10612 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:47,181 : INFO : EPOCH 4 - PROGRESS: at 86.93% examples, 10586 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:48,535 : INFO : EPOCH 4 - PROGRESS: at 87.30% examples, 10564 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:50,798 : INFO : EPOCH 4 - PROGRESS: at 88.30% examples, 10589 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:52,242 : INFO : EPOCH 4 - PROGRESS: at 88.74% examples, 10563 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:58:53,328 : INFO : EPOCH 4 - PROGRESS: at 89.57% examples, 10601 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:54,468 : INFO : EPOCH 4 - PROGRESS: at 89.92% examples, 10590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:56,092 : INFO : EPOCH 4 - PROGRESS: at 90.30% examples, 10556 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:57,395 : INFO : EPOCH 4 - PROGRESS: at 90.73% examples, 10538 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:58:59,255 : INFO : EPOCH 4 - PROGRESS: at 91.46% examples, 10537 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:01,736 : INFO : EPOCH 4 - PROGRESS: at 91.85% examples, 10463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:05,979 : INFO : EPOCH 4 - PROGRESS: at 93.13% examples, 10428 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:06,986 : INFO : EPOCH 4 - PROGRESS: at 94.43% examples, 10511 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:09,472 : INFO : EPOCH 4 - PROGRESS: at 94.60% examples, 10441 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:10,612 : INFO : EPOCH 4 - PROGRESS: at 95.71% examples, 10516 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:12,678 : INFO : EPOCH 4 - PROGRESS: at 96.02% examples, 10465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:13,741 : INFO : EPOCH 4 - PROGRESS: at 96.78% examples, 10500 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:16,059 : INFO : EPOCH 4 - PROGRESS: at 97.57% examples, 10479 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:17,075 : INFO : EPOCH 4 - PROGRESS: at 98.34% examples, 10514 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-28 13:59:19,691 : INFO : EPOCH 4 - PROGRESS: at 99.12% examples, 10481 words/s, in_qsize 3, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 13:59:19,693 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 13:59:19,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 13:59:20,249 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 13:59:20,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 13:59:20,636 : INFO : EPOCH - 4 : training on 2563578 raw words (2551096 effective words) took 241.8s, 10550 effective words/s\n",
      "2019-03-28 13:59:23,597 : INFO : EPOCH 5 - PROGRESS: at 0.32% examples, 2681 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:59:27,325 : INFO : EPOCH 5 - PROGRESS: at 1.84% examples, 6934 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:30,552 : INFO : EPOCH 5 - PROGRESS: at 2.97% examples, 8625 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:59:34,428 : INFO : EPOCH 5 - PROGRESS: at 4.63% examples, 9073 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:59:38,243 : INFO : EPOCH 5 - PROGRESS: at 6.26% examples, 9346 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:40,400 : INFO : EPOCH 5 - PROGRESS: at 7.77% examples, 10103 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:42,081 : INFO : EPOCH 5 - PROGRESS: at 8.20% examples, 9765 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:44,192 : INFO : EPOCH 5 - PROGRESS: at 9.20% examples, 10142 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:46,166 : INFO : EPOCH 5 - PROGRESS: at 9.61% examples, 9738 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 13:59:48,370 : INFO : EPOCH 5 - PROGRESS: at 10.86% examples, 10028 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:49,870 : INFO : EPOCH 5 - PROGRESS: at 11.28% examples, 9851 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:51,421 : INFO : EPOCH 5 - PROGRESS: at 12.29% examples, 10299 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:53,355 : INFO : EPOCH 5 - PROGRESS: at 12.71% examples, 9993 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:54,994 : INFO : EPOCH 5 - PROGRESS: at 13.99% examples, 10372 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:57,203 : INFO : EPOCH 5 - PROGRESS: at 14.37% examples, 10014 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 13:59:58,640 : INFO : EPOCH 5 - PROGRESS: at 15.53% examples, 10401 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:00,930 : INFO : EPOCH 5 - PROGRESS: at 15.97% examples, 10052 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:02,188 : INFO : EPOCH 5 - PROGRESS: at 17.12% examples, 10434 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:03,629 : INFO : EPOCH 5 - PROGRESS: at 17.28% examples, 10259 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:05,003 : INFO : EPOCH 5 - PROGRESS: at 17.69% examples, 10163 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:06,746 : INFO : EPOCH 5 - PROGRESS: at 18.73% examples, 10418 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:07,940 : INFO : EPOCH 5 - PROGRESS: at 19.16% examples, 10365 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:10,183 : INFO : EPOCH 5 - PROGRESS: at 20.16% examples, 10487 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:12,397 : INFO : EPOCH 5 - PROGRESS: at 20.42% examples, 10228 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:14,078 : INFO : EPOCH 5 - PROGRESS: at 21.24% examples, 10357 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:16,374 : INFO : EPOCH 5 - PROGRESS: at 21.49% examples, 10105 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:18,223 : INFO : EPOCH 5 - PROGRESS: at 22.70% examples, 10287 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:20,694 : INFO : EPOCH 5 - PROGRESS: at 23.13% examples, 10027 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:22,606 : INFO : EPOCH 5 - PROGRESS: at 24.15% examples, 10195 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:24,917 : INFO : EPOCH 5 - PROGRESS: at 24.56% examples, 9978 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:26,473 : INFO : EPOCH 5 - PROGRESS: at 25.85% examples, 10190 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:28,440 : INFO : EPOCH 5 - PROGRESS: at 26.24% examples, 10037 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:30,018 : INFO : EPOCH 5 - PROGRESS: at 27.53% examples, 10233 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:32,190 : INFO : EPOCH 5 - PROGRESS: at 27.95% examples, 10056 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:33,485 : INFO : EPOCH 5 - PROGRESS: at 29.15% examples, 10279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:34,987 : INFO : EPOCH 5 - PROGRESS: at 29.42% examples, 10166 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:36,727 : INFO : EPOCH 5 - PROGRESS: at 30.45% examples, 10315 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:38,798 : INFO : EPOCH 5 - PROGRESS: at 30.85% examples, 10169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:40,326 : INFO : EPOCH 5 - PROGRESS: at 31.88% examples, 10341 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:42,221 : INFO : EPOCH 5 - PROGRESS: at 32.33% examples, 10222 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:43,656 : INFO : EPOCH 5 - PROGRESS: at 33.48% examples, 10400 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:45,600 : INFO : EPOCH 5 - PROGRESS: at 33.85% examples, 10279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:47,107 : INFO : EPOCH 5 - PROGRESS: at 35.00% examples, 10436 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:00:49,041 : INFO : EPOCH 5 - PROGRESS: at 35.38% examples, 10320 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:50,127 : INFO : EPOCH 5 - PROGRESS: at 36.43% examples, 10523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:52,211 : INFO : EPOCH 5 - PROGRESS: at 36.85% examples, 10391 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:53,372 : INFO : EPOCH 5 - PROGRESS: at 37.90% examples, 10564 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:55,580 : INFO : EPOCH 5 - PROGRESS: at 38.25% examples, 10419 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:00:57,003 : INFO : EPOCH 5 - PROGRESS: at 39.09% examples, 10469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:00,672 : INFO : EPOCH 5 - PROGRESS: at 39.71% examples, 10279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:04,057 : INFO : EPOCH 5 - PROGRESS: at 41.33% examples, 10322 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:07,665 : INFO : EPOCH 5 - PROGRESS: at 43.02% examples, 10345 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:11,657 : INFO : EPOCH 5 - PROGRESS: at 44.63% examples, 10324 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:15,484 : INFO : EPOCH 5 - PROGRESS: at 46.33% examples, 10323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:19,685 : INFO : EPOCH 5 - PROGRESS: at 47.83% examples, 10291 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:23,475 : INFO : EPOCH 5 - PROGRESS: at 49.20% examples, 10294 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:27,432 : INFO : EPOCH 5 - PROGRESS: at 50.89% examples, 10283 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:30,983 : INFO : EPOCH 5 - PROGRESS: at 52.53% examples, 10305 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:34,450 : INFO : EPOCH 5 - PROGRESS: at 53.98% examples, 10333 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:38,226 : INFO : EPOCH 5 - PROGRESS: at 55.60% examples, 10332 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:42,138 : INFO : EPOCH 5 - PROGRESS: at 57.24% examples, 10324 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:45,710 : INFO : EPOCH 5 - PROGRESS: at 58.82% examples, 10342 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:48,509 : INFO : EPOCH 5 - PROGRESS: at 60.49% examples, 10411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:01:51,495 : INFO : EPOCH 5 - PROGRESS: at 61.93% examples, 10464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:54,278 : INFO : EPOCH 5 - PROGRESS: at 63.45% examples, 10529 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:01:57,278 : INFO : EPOCH 5 - PROGRESS: at 65.06% examples, 10578 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:00,504 : INFO : EPOCH 5 - PROGRESS: at 66.78% examples, 10611 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:03,326 : INFO : EPOCH 5 - PROGRESS: at 68.35% examples, 10668 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:06,423 : INFO : EPOCH 5 - PROGRESS: at 70.06% examples, 10705 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 14:02:08,594 : INFO : EPOCH 5 - PROGRESS: at 71.53% examples, 10778 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:09,842 : INFO : EPOCH 5 - PROGRESS: at 72.36% examples, 10814 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:12,338 : INFO : EPOCH 5 - PROGRESS: at 72.97% examples, 10770 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:13,776 : INFO : EPOCH 5 - PROGRESS: at 73.82% examples, 10795 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:17,044 : INFO : EPOCH 5 - PROGRESS: at 74.62% examples, 10706 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:18,518 : INFO : EPOCH 5 - PROGRESS: at 75.38% examples, 10728 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:21,185 : INFO : EPOCH 5 - PROGRESS: at 76.16% examples, 10677 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:22,639 : INFO : EPOCH 5 - PROGRESS: at 77.01% examples, 10698 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:24,184 : INFO : EPOCH 5 - PROGRESS: at 77.82% examples, 10715 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:25,371 : INFO : EPOCH 5 - PROGRESS: at 78.59% examples, 10751 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:27,316 : INFO : EPOCH 5 - PROGRESS: at 79.43% examples, 10771 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:29,983 : INFO : EPOCH 5 - PROGRESS: at 80.85% examples, 10827 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:32,679 : INFO : EPOCH 5 - PROGRESS: at 82.37% examples, 10878 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:33,895 : INFO : EPOCH 5 - PROGRESS: at 82.75% examples, 10859 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:36,122 : INFO : EPOCH 5 - PROGRESS: at 83.94% examples, 10886 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:37,402 : INFO : EPOCH 5 - PROGRESS: at 84.33% examples, 10866 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:39,331 : INFO : EPOCH 5 - PROGRESS: at 85.32% examples, 10906 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:40,879 : INFO : EPOCH 5 - PROGRESS: at 85.69% examples, 10871 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:42,968 : INFO : EPOCH 5 - PROGRESS: at 86.93% examples, 10903 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:44,227 : INFO : EPOCH 5 - PROGRESS: at 87.34% examples, 10883 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:46,169 : INFO : EPOCH 5 - PROGRESS: at 88.30% examples, 10922 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:47,280 : INFO : EPOCH 5 - PROGRESS: at 88.74% examples, 10910 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:48,909 : INFO : EPOCH 5 - PROGRESS: at 89.92% examples, 10966 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:50,268 : INFO : EPOCH 5 - PROGRESS: at 90.30% examples, 10942 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:51,910 : INFO : EPOCH 5 - PROGRESS: at 91.46% examples, 10995 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:53,423 : INFO : EPOCH 5 - PROGRESS: at 91.87% examples, 10963 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:02:58,184 : INFO : EPOCH 5 - PROGRESS: at 93.13% examples, 10890 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:02:59,195 : INFO : EPOCH 5 - PROGRESS: at 94.43% examples, 10974 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:01,193 : INFO : EPOCH 5 - PROGRESS: at 94.60% examples, 10920 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:02,275 : INFO : EPOCH 5 - PROGRESS: at 95.71% examples, 10999 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:04,351 : INFO : EPOCH 5 - PROGRESS: at 96.02% examples, 10941 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:05,369 : INFO : EPOCH 5 - PROGRESS: at 97.17% examples, 11021 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:08,068 : INFO : EPOCH 5 - PROGRESS: at 97.57% examples, 10932 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:09,697 : INFO : EPOCH 5 - PROGRESS: at 98.72% examples, 10980 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-28 14:03:12,504 : INFO : EPOCH 5 - PROGRESS: at 99.02% examples, 10878 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 14:03:12,506 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 14:03:12,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 14:03:13,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 14:03:13,643 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 10950 words/s, in_qsize 0, out_qsize 1\n",
      "2019-03-28 14:03:13,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 14:03:13,647 : INFO : EPOCH - 5 : training on 2563578 raw words (2551187 effective words) took 233.0s, 10950 effective words/s\n",
      "2019-03-28 14:03:13,649 : INFO : training on a 12817890 raw words (12755803 effective words) took 1138.3s, 11206 effective words/s\n",
      "2019-03-28 14:03:13,651 : INFO : training model with 4 workers on 27512 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-03-28 14:03:17,796 : INFO : EPOCH 1 - PROGRESS: at 0.32% examples, 1916 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:18,910 : INFO : EPOCH 1 - PROGRESS: at 1.66% examples, 7174 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:22,369 : INFO : EPOCH 1 - PROGRESS: at 1.89% examples, 5436 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:25,689 : INFO : EPOCH 1 - PROGRESS: at 2.97% examples, 7109 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:28,870 : INFO : EPOCH 1 - PROGRESS: at 4.62% examples, 8213 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:31,830 : INFO : EPOCH 1 - PROGRESS: at 6.26% examples, 9058 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:33,383 : INFO : EPOCH 1 - PROGRESS: at 7.77% examples, 10126 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:34,412 : INFO : EPOCH 1 - PROGRESS: at 8.60% examples, 10564 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:35,714 : INFO : EPOCH 1 - PROGRESS: at 9.20% examples, 10834 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:37,100 : INFO : EPOCH 1 - PROGRESS: at 9.59% examples, 10610 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:38,825 : INFO : EPOCH 1 - PROGRESS: at 10.86% examples, 11052 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:39,870 : INFO : EPOCH 1 - PROGRESS: at 11.28% examples, 10989 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:41,336 : INFO : EPOCH 1 - PROGRESS: at 12.29% examples, 11457 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:42,631 : INFO : EPOCH 1 - PROGRESS: at 12.71% examples, 11287 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:03:44,227 : INFO : EPOCH 1 - PROGRESS: at 13.99% examples, 11660 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:45,807 : INFO : EPOCH 1 - PROGRESS: at 14.38% examples, 11388 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:47,156 : INFO : EPOCH 1 - PROGRESS: at 15.53% examples, 11801 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:48,752 : INFO : EPOCH 1 - PROGRESS: at 15.97% examples, 11543 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:49,955 : INFO : EPOCH 1 - PROGRESS: at 17.12% examples, 11946 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:51,938 : INFO : EPOCH 1 - PROGRESS: at 17.69% examples, 11781 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:53,058 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 11948 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:55,062 : INFO : EPOCH 1 - PROGRESS: at 19.16% examples, 11843 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:56,105 : INFO : EPOCH 1 - PROGRESS: at 19.93% examples, 12013 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:57,688 : INFO : EPOCH 1 - PROGRESS: at 20.42% examples, 12025 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:03:58,706 : INFO : EPOCH 1 - PROGRESS: at 21.24% examples, 12287 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:04:00,143 : INFO : EPOCH 1 - PROGRESS: at 21.49% examples, 12118 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:01,382 : INFO : EPOCH 1 - PROGRESS: at 22.70% examples, 12415 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:02,700 : INFO : EPOCH 1 - PROGRESS: at 23.13% examples, 12281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:03,891 : INFO : EPOCH 1 - PROGRESS: at 24.15% examples, 12579 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:05,550 : INFO : EPOCH 1 - PROGRESS: at 24.55% examples, 12368 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:07,180 : INFO : EPOCH 1 - PROGRESS: at 25.85% examples, 12536 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 14:04:09,168 : INFO : EPOCH 1 - PROGRESS: at 26.30% examples, 12265 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:10,876 : INFO : EPOCH 1 - PROGRESS: at 27.53% examples, 12409 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:13,410 : INFO : EPOCH 1 - PROGRESS: at 27.95% examples, 12043 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:14,844 : INFO : EPOCH 1 - PROGRESS: at 29.15% examples, 12239 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:04:16,350 : INFO : EPOCH 1 - PROGRESS: at 29.42% examples, 12058 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:17,898 : INFO : EPOCH 1 - PROGRESS: at 30.45% examples, 12219 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:19,569 : INFO : EPOCH 1 - PROGRESS: at 30.85% examples, 12059 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:22,128 : INFO : EPOCH 1 - PROGRESS: at 32.33% examples, 12180 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:24,726 : INFO : EPOCH 1 - PROGRESS: at 33.84% examples, 12284 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:27,304 : INFO : EPOCH 1 - PROGRESS: at 35.38% examples, 12388 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:29,696 : INFO : EPOCH 1 - PROGRESS: at 36.85% examples, 12510 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:32,220 : INFO : EPOCH 1 - PROGRESS: at 38.25% examples, 12592 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:34,980 : INFO : EPOCH 1 - PROGRESS: at 39.71% examples, 12645 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:38,302 : INFO : EPOCH 1 - PROGRESS: at 41.33% examples, 12612 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:41,765 : INFO : EPOCH 1 - PROGRESS: at 43.02% examples, 12567 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:45,563 : INFO : EPOCH 1 - PROGRESS: at 44.63% examples, 12472 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:04:48,751 : INFO : EPOCH 1 - PROGRESS: at 46.33% examples, 12468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:51,304 : INFO : EPOCH 1 - PROGRESS: at 47.83% examples, 12547 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:53,831 : INFO : EPOCH 1 - PROGRESS: at 49.20% examples, 12623 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:04:57,339 : INFO : EPOCH 1 - PROGRESS: at 50.85% examples, 12574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:00,191 : INFO : EPOCH 1 - PROGRESS: at 52.53% examples, 12609 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:02,920 : INFO : EPOCH 1 - PROGRESS: at 53.98% examples, 12654 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:05,386 : INFO : EPOCH 1 - PROGRESS: at 55.60% examples, 12724 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:08,618 : INFO : EPOCH 1 - PROGRESS: at 57.24% examples, 12708 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:12,045 : INFO : EPOCH 1 - PROGRESS: at 58.82% examples, 12674 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:15,221 : INFO : EPOCH 1 - PROGRESS: at 60.49% examples, 12664 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:18,228 : INFO : EPOCH 1 - PROGRESS: at 61.93% examples, 12672 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:20,910 : INFO : EPOCH 1 - PROGRESS: at 63.45% examples, 12712 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:23,547 : INFO : EPOCH 1 - PROGRESS: at 65.06% examples, 12757 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:26,382 : INFO : EPOCH 1 - PROGRESS: at 66.78% examples, 12781 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:29,193 : INFO : EPOCH 1 - PROGRESS: at 68.37% examples, 12803 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:32,031 : INFO : EPOCH 1 - PROGRESS: at 70.06% examples, 12826 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:34,258 : INFO : EPOCH 1 - PROGRESS: at 71.53% examples, 12875 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:35,500 : INFO : EPOCH 1 - PROGRESS: at 72.36% examples, 12900 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:37,884 : INFO : EPOCH 1 - PROGRESS: at 72.97% examples, 12822 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:38,946 : INFO : EPOCH 1 - PROGRESS: at 73.40% examples, 12796 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:41,620 : INFO : EPOCH 1 - PROGRESS: at 74.62% examples, 12764 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:42,623 : INFO : EPOCH 1 - PROGRESS: at 74.99% examples, 12745 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:45,323 : INFO : EPOCH 1 - PROGRESS: at 76.16% examples, 12710 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:46,595 : INFO : EPOCH 1 - PROGRESS: at 76.61% examples, 12667 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:49,222 : INFO : EPOCH 1 - PROGRESS: at 77.82% examples, 12642 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:50,465 : INFO : EPOCH 1 - PROGRESS: at 78.24% examples, 12605 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:53,725 : INFO : EPOCH 1 - PROGRESS: at 79.43% examples, 12562 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:05:57,354 : INFO : EPOCH 1 - PROGRESS: at 80.85% examples, 12523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:05:58,495 : INFO : EPOCH 1 - PROGRESS: at 81.70% examples, 12556 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:01,182 : INFO : EPOCH 1 - PROGRESS: at 82.37% examples, 12469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:02,722 : INFO : EPOCH 1 - PROGRESS: at 82.75% examples, 12413 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:06,530 : INFO : EPOCH 1 - PROGRESS: at 83.94% examples, 12310 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:07,796 : INFO : EPOCH 1 - PROGRESS: at 84.33% examples, 12277 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:10,133 : INFO : EPOCH 1 - PROGRESS: at 85.32% examples, 12279 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:11,998 : INFO : EPOCH 1 - PROGRESS: at 85.69% examples, 12206 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:15,016 : INFO : EPOCH 1 - PROGRESS: at 86.93% examples, 12164 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:16,311 : INFO : EPOCH 1 - PROGRESS: at 87.34% examples, 12130 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:19,116 : INFO : EPOCH 1 - PROGRESS: at 88.30% examples, 12104 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:20,379 : INFO : EPOCH 1 - PROGRESS: at 89.18% examples, 12128 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:22,244 : INFO : EPOCH 1 - PROGRESS: at 89.92% examples, 12110 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:23,382 : INFO : EPOCH 1 - PROGRESS: at 90.30% examples, 12090 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:25,528 : INFO : EPOCH 1 - PROGRESS: at 91.46% examples, 12106 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:26,941 : INFO : EPOCH 1 - PROGRESS: at 91.85% examples, 12068 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:30,916 : INFO : EPOCH 1 - PROGRESS: at 93.13% examples, 12009 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:33,458 : INFO : EPOCH 1 - PROGRESS: at 94.60% examples, 12054 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:34,614 : INFO : EPOCH 1 - PROGRESS: at 95.71% examples, 12130 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:37,318 : INFO : EPOCH 1 - PROGRESS: at 96.02% examples, 12018 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-28 14:06:38,322 : INFO : EPOCH 1 - PROGRESS: at 97.17% examples, 12101 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:41,537 : INFO : EPOCH 1 - PROGRESS: at 97.57% examples, 11960 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-28 14:06:42,890 : INFO : EPOCH 1 - PROGRESS: at 98.34% examples, 11973 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-28 14:06:46,873 : INFO : EPOCH 1 - PROGRESS: at 99.02% examples, 11829 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 14:06:46,875 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 14:06:47,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 14:06:47,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 14:06:47,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 14:06:47,783 : INFO : EPOCH - 1 : training on 2563578 raw words (2551105 effective words) took 214.1s, 11915 effective words/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-18d8d0ab7f13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_train_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m0.002\u001b[0m \u001b[1;31m# decrease the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;31m# fix the learning rate, no decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start = time()\n",
    "# for epoch in range(5):\n",
    "#     doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "#     doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n",
    "#     doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n",
    "# end = time()\n",
    "# print(\"During Time: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Doc2vec1.model'\n",
    "# doc_vectorizer.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-19 16:40:21,988 : INFO : loading Doc2Vec object from Doc2vec1.model\n",
      "2019-10-19 16:40:23,642 : INFO : loading vocabulary recursively from Doc2vec1.model.vocabulary.* with mmap=None\n",
      "2019-10-19 16:40:23,643 : INFO : loading trainables recursively from Doc2vec1.model.trainables.* with mmap=None\n",
      "2019-10-19 16:40:23,644 : INFO : loading wv recursively from Doc2vec1.model.wv.* with mmap=None\n",
      "2019-10-19 16:40:23,646 : INFO : loading docvecs recursively from Doc2vec1.model.docvecs.* with mmap=None\n",
      "2019-10-19 16:40:23,647 : INFO : loaded Doc2vec1.model\n"
     ]
    }
   ],
   "source": [
    "# loading Doc2vec model\n",
    "model_name = 'Doc2vec1.model'\n",
    "doc_vectorizer = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-19 16:40:24,349 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('벙커링', 0.6494147181510925), ('연료추진선', 0.6215590238571167), ('운반선', 0.5957974195480347), ('LNG연료공급', 0.5679702758789062), ('LNG연료추진선', 0.567599892616272), ('VLCC', 0.563372790813446), ('LNG벙커링', 0.5483577847480774), ('추진선박', 0.5353018641471863), ('LNG연료', 0.5342037677764893), ('추진선', 0.5339316725730896)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectorizer.wv.most_similar('LNG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('추진시스템', 0.3174276351928711), ('추진계통', 0.31573396921157837), ('해양사고', 0.30571871995925903), ('전류고정날개', 0.3017979860305786), ('성능해석기술', 0.29743826389312744), ('시설재배', 0.2963535189628601), ('예인선', 0.2923802137374878), ('기존선박', 0.29178160429000854), ('드릴쉽', 0.288592666387558), ('추진선', 0.2880726754665375)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectorizer.wv.most_similar(positive=['선박', '스마트'], negative=['금속']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "y_train = [doc.tags for doc in tagged_train_docs]\n",
    "X_test  = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "y_test  = [doc.tags for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>big_class</th>\n",
       "      <th>small_class</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>big_calss_re</th>\n",
       "      <th>new_class</th>\n",
       "      <th>new_small_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "      <td>개요 가  정의 및 필요성   차세대 드론 개발 기술은 무선전파로 조정할 ...</td>\n",
       "      <td>[개요, 정의, 필요성, 차세대, 드론, 무선전파, 조정, 무인항공기, 드론, 위하...</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "      <td>나  범위     제품분류 관점   드론은 사용 목적  탑재되는 임무장비에 따라...</td>\n",
       "      <td>[범위, 제품분류, 관점, 드론, 목적, 탑재, 임무장비, 분류, 드론, 구성, 핵...</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "      <td>외부환경 분석 가  산업환경 분석     산업의 특징   드론 산업은  차...</td>\n",
       "      <td>[외부환경, 산업환경, 특징, 드론, 집약체, AI, 자율지능, 프린팅, 연료전지,...</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "      <td>나  시장환경 분석     세계시장   미국 방위산업 전문 컨설팅 업체인 틸그룹...</td>\n",
       "      <td>[시장환경, 세계시장, 방위산업, 전문, 컨설팅, 업체, Group, 세계, 드론,...</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "      <td>무역현황   무역현황은 드론과 관련된 무역현황으로 살펴보았으며  ‘  년...</td>\n",
       "      <td>[무역현황, 무역현황, 드론, 무역현황, 이후, 수출량, 줄어, 수출량, 늘어, 추...</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>항공우주</td>\n",
       "      <td>차세대 드론 개발 기술</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year big_class   small_class  \\\n",
       "0  2017      항공우주  차세대 드론 개발 기술   \n",
       "1  2017      항공우주  차세대 드론 개발 기술   \n",
       "2  2017      항공우주  차세대 드론 개발 기술   \n",
       "3  2017      항공우주  차세대 드론 개발 기술   \n",
       "4  2017      항공우주  차세대 드론 개발 기술   \n",
       "\n",
       "                                             content  \\\n",
       "0       개요 가  정의 및 필요성   차세대 드론 개발 기술은 무선전파로 조정할 ...   \n",
       "1    나  범위     제품분류 관점   드론은 사용 목적  탑재되는 임무장비에 따라...   \n",
       "2       외부환경 분석 가  산업환경 분석     산업의 특징   드론 산업은  차...   \n",
       "3    나  시장환경 분석     세계시장   미국 방위산업 전문 컨설팅 업체인 틸그룹...   \n",
       "4        무역현황   무역현황은 드론과 관련된 무역현황으로 살펴보았으며  ‘  년...   \n",
       "\n",
       "                                               token big_calss_re new_class  \\\n",
       "0  [개요, 정의, 필요성, 차세대, 드론, 무선전파, 조정, 무인항공기, 드론, 위하...         항공우주      항공우주   \n",
       "1  [범위, 제품분류, 관점, 드론, 목적, 탑재, 임무장비, 분류, 드론, 구성, 핵...         항공우주      항공우주   \n",
       "2  [외부환경, 산업환경, 특징, 드론, 집약체, AI, 자율지능, 프린팅, 연료전지,...         항공우주      항공우주   \n",
       "3  [시장환경, 세계시장, 방위산업, 전문, 컨설팅, 업체, Group, 세계, 드론,...         항공우주      항공우주   \n",
       "4  [무역현황, 무역현황, 드론, 무역현황, 이후, 수출량, 줄어, 수출량, 늘어, 추...         항공우주      항공우주   \n",
       "\n",
       "  new_small_class  \n",
       "0    차세대 드론 개발 기술  \n",
       "1    차세대 드론 개발 기술  \n",
       "2    차세대 드론 개발 기술  \n",
       "3    차세대 드론 개발 기술  \n",
       "4    차세대 드론 개발 기술  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dic = {}\n",
    "big = list(set(doc_set['new_class']))\n",
    "for i in range(len(big)):\n",
    "    temp = big[i]\n",
    "    s_temp = list (set(doc_set[doc_set['new_class']==temp].new_small_class))\n",
    "    category_dic[temp]=s_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7138 3060\n"
     ]
    }
   ],
   "source": [
    "label_type = 'new_small_class'\n",
    "df_train, df_test = train_test_split(doc_set, test_size=0.3,random_state=42)\n",
    "print(len(df_train),len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument(d, c) for d, c in df_train[['token', 'new_class']].values]\n",
    "tagged_test_docs = [TaggedDocument(d, c) for d, c in df_test[['token', 'new_class']].values]\n",
    "X_train = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "y_train = [doc.tags for doc in tagged_train_docs]\n",
    "X_test  = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "y_test  = [doc.tags for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train_docs2 = [TaggedDocument(d, c) for d, c in df_train[['token', 'new_small_class']].values]\n",
    "tagged_test_docs2 = [TaggedDocument(d, c) for d, c in df_test[['token', 'new_small_class']].values]\n",
    "y_train2 = [doc.tags for doc in tagged_train_docs2]\n",
    "y_test2  = [doc.tags for doc in tagged_test_docs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(1000,), max_iter=10, alpha=1e-4, solver='sgd',\n",
    "                        verbose=10, tol=1e-4, random_state=123,learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf2 = MLPClassifier(\n",
    "    hidden_layer_sizes=(1000,),\n",
    "    max_iter=20,\n",
    "    alpha=1e-4,\n",
    "    solver='sgd',\n",
    "    verbose=10,\n",
    "    tol=1e-4,\n",
    "    random_state=123,\n",
    "    learning_rate_init=.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.54369531\n",
      "Iteration 2, loss = 0.80737445\n",
      "Iteration 3, loss = 0.41704808\n",
      "Iteration 4, loss = 0.28498373\n",
      "Iteration 5, loss = 0.21531149\n",
      "Iteration 6, loss = 0.16715184\n",
      "Iteration 7, loss = 0.13041851\n",
      "Iteration 8, loss = 0.10757112\n",
      "Iteration 9, loss = 0.08695456\n",
      "Iteration 10, loss = 0.07156690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1class 정확도: 0.853\n",
      "3class 정확도: 0.9640522875816994\n"
     ]
    }
   ],
   "source": [
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp_clf.predict(X_test)\n",
    "print(\"1class 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))\n",
    "y_pred_prob = mlp_clf.predict_proba(X_test)\n",
    "L = np.argsort(-y_pred_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {mlp_clf.classes_[i]: i for i in range(len(mlp_clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "score = []\n",
    "for i in range(len(y_test)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    third = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(third)]])\n",
    "    if y_test[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    dd.append({'y_test':y_test[i], 'first':label[0], 'second': label[1],'third': label[2]})\n",
    "acc = sum(score)/len(y_test)\n",
    "print(\"3class 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.89942825\n",
      "Iteration 2, loss = 4.74950241\n",
      "Iteration 3, loss = 3.10452287\n",
      "Iteration 4, loss = 1.75152436\n",
      "Iteration 5, loss = 1.00247582\n",
      "Iteration 6, loss = 0.62821776\n",
      "Iteration 7, loss = 0.43654928\n",
      "Iteration 8, loss = 0.31564458\n",
      "Iteration 9, loss = 0.23746317\n",
      "Iteration 10, loss = 0.18762772\n",
      "Iteration 11, loss = 0.14829516\n",
      "Iteration 12, loss = 0.12536452\n",
      "Iteration 13, loss = 0.10329625\n",
      "Iteration 14, loss = 0.08991689\n",
      "Iteration 15, loss = 0.07822615\n",
      "Iteration 16, loss = 0.06910897\n",
      "Iteration 17, loss = 0.06293344\n",
      "Iteration 18, loss = 0.05641129\n",
      "Iteration 19, loss = 0.05240951\n",
      "Iteration 20, loss = 0.04863279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_clf2.fit(X_train, y_train2)\n",
    "y_pred = mlp_clf2.predict(X_test)\n",
    "y_pred_prob = mlp_clf2.predict_proba(X_test)\n",
    "L = np.argsort(-y_pred_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "class_dic = {mlp_clf2.classes_[i]: i for i in range(len(mlp_clf2.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result = []\n",
    "score = []\n",
    "for i in range(len(y_test2)):\n",
    "    tt = category_dic[dd[i]['first']]+category_dic[dd[i]['second']]\n",
    "    tt = [x for x in tt if x in key_list]\n",
    "    ttt = list({ your_key: class_dic[your_key] for your_key in tt }.values())\n",
    "    tttt = [x for x in L[i] if x in ttt]\n",
    "    first = tttt[0]\n",
    "    second = tttt[1]\n",
    "    third = tttt[2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(third)]])\n",
    "    if y_test2[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    second_result.append({'y_test':y_test2[i], 'first':label[0], 'second': label[1],'third': label[2]})\n",
    "acc = sum(score)/len(second_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8797385620915033"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3class 정확도: 0.8209150326797385\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=1234)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"1class 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))\n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "L = np.argsort(-y_pred_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {clf.classes_[i]: i for i in range(len(clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "score = []\n",
    "for i in range(len(y_test)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    third = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(third)]])\n",
    "    if y_test[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    dd.append({'y_test':y_test[i], 'first':label[0], 'second': label[1],'third': label[2]})\n",
    "acc = sum(score)/len(y_test)\n",
    "print(\"3class 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.91254410\n",
      "Iteration 2, loss = 4.75403352\n",
      "Iteration 3, loss = 3.11628984\n",
      "Iteration 4, loss = 1.75860231\n",
      "Iteration 5, loss = 1.01270443\n",
      "Iteration 6, loss = 0.63180073\n",
      "Iteration 7, loss = 0.43230865\n",
      "Iteration 8, loss = 0.31561289\n",
      "Iteration 9, loss = 0.23740707\n",
      "Iteration 10, loss = 0.18648184\n",
      "Iteration 11, loss = 0.15067683\n",
      "Iteration 12, loss = 0.12489761\n",
      "Iteration 13, loss = 0.10358679\n",
      "Iteration 14, loss = 0.08960562\n",
      "Iteration 15, loss = 0.07758507\n",
      "Iteration 16, loss = 0.06901886\n",
      "Iteration 17, loss = 0.06292978\n",
      "Iteration 18, loss = 0.05487183\n",
      "Iteration 19, loss = 0.05163544\n",
      "Iteration 20, loss = 0.04740343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1class 정확도: 0.738\n",
      "3class 정확도: 0.9035317200784827\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(1000,),\n",
    "    max_iter=15,\n",
    "    alpha=1e-4,\n",
    "    solver='sgd',\n",
    "    verbose=10,\n",
    "    tol=1e-4,\n",
    "    random_state=123,\n",
    "    learning_rate_init=.1\n",
    ")\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp_clf.predict(X_test)\n",
    "print(\"1class 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))\n",
    "y_pred_prob = mlp_clf.predict_proba(X_test)\n",
    "L = np.argsort(-y_pred_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {mlp_clf.classes_[i]: i for i in range(len(mlp_clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "score = []\n",
    "for i in range(len(y_test)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    third = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(third)]])\n",
    "    if y_test[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    dd.append({'y_test':y_test[i], 'first':label[0], 'second': label[1],'third': label[2]})\n",
    "acc = sum(score)/len(y_test)\n",
    "print(\"3class 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calssification start-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>company</th>\n",
       "      <th>invest_num</th>\n",
       "      <th>overview</th>\n",
       "      <th>sub_info</th>\n",
       "      <th>naver_category</th>\n",
       "      <th>naver_detail</th>\n",
       "      <th>intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...</td>\n",
       "      <td>쿠팡</td>\n",
       "      <td>3</td>\n",
       "      <td>\"쿠팡은 세계에서 가장 빠르고 크게 성장하는 E-commerce 기업입니다. 우리는...</td>\n",
       "      <td>['설립일:              2010-07-01\\xa0/\\xa09년차', '...</td>\n",
       "      <td>이커머스, 쇼핑몰, 마트, 패션의류/잡화, 뷰티, 여행/레저, 할인.</td>\n",
       "      <td>{'기업명': '쿠팡(주)', '기업구분': '중견기업', '대표자': '범석김',...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['마케팅', '스타트업', 'startup']</td>\n",
       "      <td>KOTRA 수출창업지원팀</td>\n",
       "      <td>1</td>\n",
       "      <td>국내 스타트업 기업의 글로벌 시장 진출 지원  \\n\\n\\n\\n</td>\n",
       "      <td>['투자유치:              누적 6,000억 원 이상        상세보...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['온라인게임']</td>\n",
       "      <td>넷마블게임즈</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>['투자유치:              누적 5,300억 원 이상        상세보...</td>\n",
       "      <td>2011년 11월 설립된 동사는 개발 스튜디오에서 게임을 직접 개발하고, PC/모바...</td>\n",
       "      <td>{'기업명': '넷마블(주)', '기업구분': '중견기업, 코스피 상장', '대표자...</td>\n",
       "      <td>\\n            씨제이넷마블(주)와 씨제이게임즈(주)가 넷마블게임즈(주)로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['모바일광고', '모바일광고플랫폼', '애드테크', '애드네트워크', 'DSP',...</td>\n",
       "      <td>Avazu Korea</td>\n",
       "      <td>1</td>\n",
       "      <td>글로벌 유니콘 스타트업이 한국에 진출합니다! About usDotC United G...</td>\n",
       "      <td>['구성원:              501-1000명        상세보기', '투...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['통신']</td>\n",
       "      <td>SK브로드밴드</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>['설립일:              1997-09-26\\xa0/\\xa022년차', ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n            에스케이브로드밴드(주)는 케이블 TV서비스 및 초고속 인터...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category        company  \\\n",
       "0  ['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...             쿠팡   \n",
       "1                         ['마케팅', '스타트업', 'startup']  KOTRA 수출창업지원팀   \n",
       "2                                          ['온라인게임']         넷마블게임즈   \n",
       "3  ['모바일광고', '모바일광고플랫폼', '애드테크', '애드네트워크', 'DSP',...    Avazu Korea   \n",
       "4                                             ['통신']        SK브로드밴드   \n",
       "\n",
       "   invest_num                                           overview  \\\n",
       "0           3  \"쿠팡은 세계에서 가장 빠르고 크게 성장하는 E-commerce 기업입니다. 우리는...   \n",
       "1           1                 국내 스타트업 기업의 글로벌 시장 진출 지원  \\n\\n\\n\\n   \n",
       "2           1                                               none   \n",
       "3           1  글로벌 유니콘 스타트업이 한국에 진출합니다! About usDotC United G...   \n",
       "4           2                                               none   \n",
       "\n",
       "                                            sub_info  \\\n",
       "0  ['설립일:              2010-07-01\\xa0/\\xa09년차', '...   \n",
       "1  ['투자유치:              누적 6,000억 원 이상        상세보...   \n",
       "2  ['투자유치:              누적 5,300억 원 이상        상세보...   \n",
       "3  ['구성원:              501-1000명        상세보기', '투...   \n",
       "4  ['설립일:              1997-09-26\\xa0/\\xa022년차', ...   \n",
       "\n",
       "                                      naver_category  \\\n",
       "0             이커머스, 쇼핑몰, 마트, 패션의류/잡화, 뷰티, 여행/레저, 할인.   \n",
       "1                                               None   \n",
       "2  2011년 11월 설립된 동사는 개발 스튜디오에서 게임을 직접 개발하고, PC/모바...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                        naver_detail  \\\n",
       "0  {'기업명': '쿠팡(주)', '기업구분': '중견기업', '대표자': '범석김',...   \n",
       "1                                               None   \n",
       "2  {'기업명': '넷마블(주)', '기업구분': '중견기업, 코스피 상장', '대표자...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                               intro  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  \\n            씨제이넷마블(주)와 씨제이게임즈(주)가 넷마블게임즈(주)로...  \n",
       "3                                                     \n",
       "4  \\n            에스케이브로드밴드(주)는 케이블 TV서비스 및 초고속 인터...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_list = pd.read_csv('C:/Users/hangy/Desktop/topics/data/company_list.csv',encoding='utf-8-sig',engine='python')\n",
    "company_list = company_list.replace(np.nan, '', regex=True)\n",
    "company_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 58194 from 2454 sents. mem=1.168 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=244906, mem=1.266 Gb\n",
      "[Noun Extractor] batch prediction was completed for 19105 words\n",
      "[Noun Extractor] checked compounds. discovered 8021 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 14425 -> 12844\n",
      "[Noun Extractor] postprocessing ignore_features : 12844 -> 12804\n",
      "[Noun Extractor] postprocessing ignore_NJ : 12804 -> 12769\n",
      "[Noun Extractor] 12769 nouns (8021 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.295 Gb                    \n",
      "[Noun Extractor] 66.75 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "document = []\n",
    "for i in range(len(company_list)):\n",
    "    if ('기업 소개가 없습니다' in company_list['overview'][i]) or (company_list['overview'][i] == 'none'): \n",
    "        document.append(company_list['category'][i])\n",
    "    else :\n",
    "        document.append(company_list['category'][i] + company_list['overview'][i])\n",
    "        \n",
    "document = []\n",
    "for i in range(len(company_list)):\n",
    "    document.append(company_list['category'][i] + company_list['overview'][i]+company_list['naver_category'][i]+company_list['intro'][i])\n",
    "    \n",
    "for i in range(len(document)):\n",
    "    document[i] = re.sub('[\\n\\uf000\\x0c()/▪,.ㆍ\\[\\]0-9\\'●※\\\"▷▶:\\};&\\-><%|🌟✔️]',' ', document[i])\n",
    "    \n",
    "company_data = pd.DataFrame({'company':company_list['company'],'category':company_list['category'],\n",
    "                             'document':document,'lable':['']*len(document)})\n",
    "\n",
    "company_corpus = noun_corpus(company_data['document'])\n",
    "company_docs = stopwords(stops, company_corpus)\n",
    "company_data['token'] = company_docs\n",
    "\n",
    "stops = list(ko_stopwords['stopwords'])\n",
    "company_docs = stopwords(stops, company_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(ko_stopwords['stopwords'])\n",
    "company_docs = stopwords(stops, company_docs)\n",
    "\n",
    "company_data['token'] = company_docs\n",
    "company_tokens = [ t for d in company_data['token'] for t in d]\n",
    "company_text = nltk.Text(company_tokens, name='NMSC')\n",
    "company_fdist = company_text.vocab()\n",
    "\n",
    "company_fdist = pd.DataFrame.from_dict(company_fdist, orient='index')\n",
    "company_fdist.columns = ['frequency']\n",
    "company_fdist['term'] = list(company_fdist.index)\n",
    "company_fdist = company_fdist.reset_index(drop=True)\n",
    "company_fdist = company_fdist.sort_values([\"frequency\"], ascending=[False])\n",
    "company_fdist = company_fdist.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zif'slow \n",
    "zif_list = pd.DataFrame(company_fdist[(company_fdist['frequency'] >500) | (company_fdist['frequency'] <= 2)]['term'])\n",
    "zif_list.columns = ['stopwords']\n",
    "zif_list = zif_list.reset_index(drop=True)\n",
    "\n",
    "stops = list(zif_list['stopwords'])\n",
    "company_docs2 = stopwords(stops, company_docs)\n",
    "company_data['token'] = company_docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len =[]\n",
    "for i in range(len(company_data)):\n",
    "    token_len.append(len(company_data['token'][i]))\n",
    "company_data['token_len'] = token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_company = company_data[company_data['token_len']>41].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>category</th>\n",
       "      <th>document</th>\n",
       "      <th>lable</th>\n",
       "      <th>token</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>쿠팡</td>\n",
       "      <td>['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...</td>\n",
       "      <td>모바일서비스    웹서비스    마케팅    소프트웨어    데이터    e c...</td>\n",
       "      <td></td>\n",
       "      <td>[commerce, 소셜커머스, 배송, 유통업, 쿠팡, 세계, commerce, 쿠...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울반도체</td>\n",
       "      <td>['제조업']</td>\n",
       "      <td>제조업  none일반조명  IT  자동차  UV 등 광범위한 분야에 적용되는 L...</td>\n",
       "      <td></td>\n",
       "      <td>[제조업, IT, 자동차, 광범위, 분야, 적용, LED, 제품, 연구개발, 생산,...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>롯데하이마트</td>\n",
       "      <td>['유통', '전자제품']</td>\n",
       "      <td>유통    전자제품  none가전제품 도 소매업을 영위할 목적으로     년  ...</td>\n",
       "      <td></td>\n",
       "      <td>[유통, 전자제품, 소매, 영위, 목적, 물류센터, 운영, 있음, 가전제품, 전자제...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>옐로모바일</td>\n",
       "      <td>['모바일서비스', '웹서비스', '소프트웨어', '모바일게임', '소셜커머스', ...</td>\n",
       "      <td>모바일서비스    웹서비스    소프트웨어    모바일게임    소셜커머스   ...</td>\n",
       "      <td></td>\n",
       "      <td>[모바일게임, 소셜커머스, 오픈마켓, 옐로모바일, 옐로모바일, 먹고, 일상, 라이프...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미미박스</td>\n",
       "      <td>['Confluence', 'JIRA', 'Google Apps', 'Slack',...</td>\n",
       "      <td>Confluence    JIRA    Google Apps    Slack  ...</td>\n",
       "      <td></td>\n",
       "      <td>[Confluence, commerce, io, 안드로이드, 개발자, 뷰티스타트업,...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                           category  \\\n",
       "0      쿠팡  ['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...   \n",
       "1   서울반도체                                            ['제조업']   \n",
       "2  롯데하이마트                                     ['유통', '전자제품']   \n",
       "3   옐로모바일  ['모바일서비스', '웹서비스', '소프트웨어', '모바일게임', '소셜커머스', ...   \n",
       "4    미미박스  ['Confluence', 'JIRA', 'Google Apps', 'Slack',...   \n",
       "\n",
       "                                            document lable  \\\n",
       "0    모바일서비스    웹서비스    마케팅    소프트웨어    데이터    e c...         \n",
       "1    제조업  none일반조명  IT  자동차  UV 등 광범위한 분야에 적용되는 L...         \n",
       "2    유통    전자제품  none가전제품 도 소매업을 영위할 목적으로     년  ...         \n",
       "3    모바일서비스    웹서비스    소프트웨어    모바일게임    소셜커머스   ...         \n",
       "4    Confluence    JIRA    Google Apps    Slack  ...         \n",
       "\n",
       "                                               token  token_len  \n",
       "0  [commerce, 소셜커머스, 배송, 유통업, 쿠팡, 세계, commerce, 쿠...         67  \n",
       "1  [제조업, IT, 자동차, 광범위, 분야, 적용, LED, 제품, 연구개발, 생산,...         90  \n",
       "2  [유통, 전자제품, 소매, 영위, 목적, 물류센터, 운영, 있음, 가전제품, 전자제...         54  \n",
       "3  [모바일게임, 소셜커머스, 오픈마켓, 옐로모바일, 옐로모바일, 먹고, 일상, 라이프...         44  \n",
       "4  [Confluence, commerce, io, 안드로이드, 개발자, 뷰티스타트업,...         99  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_company.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_company_docs = [TaggedDocument(d, c) for d, c in sub_company[['token', 'lable']].values]\n",
    "X_company = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_company_docs]\n",
    "y_company = [doc.tags for doc in tagged_company_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_company_pred = mlp_clf.predict(X_company)\n",
    "y_company_prob = mlp_clf.predict_proba(X_company)\n",
    "L = np.argsort(-y_company_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {clf.classes_[i]: i for i in range(len(clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "\n",
    "for i in range(len(y_company_prob)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    thrid = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(thrid)]])\n",
    "    dd.append({'company':sub_company['company'][i],'origianl_label':sub_company['category'][i],\n",
    "               'predicted_label1':label[0], 'predicted_label2': label[1], 'predicted_label3': label[2]})\n",
    "\n",
    "company_pred_df = pd.DataFrame(dd,columns = ['company','origianl_label','predicted_label1','predicted_label2','predicted_label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_pred_df.to_excel('C:/Users/hangy/Desktop/company_predict_result.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>origianl_label</th>\n",
       "      <th>predicted_label1</th>\n",
       "      <th>predicted_label2</th>\n",
       "      <th>predicted_label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>쿠팡</td>\n",
       "      <td>['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울반도체</td>\n",
       "      <td>['제조업']</td>\n",
       "      <td>LED/광</td>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>전자부품</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>롯데하이마트</td>\n",
       "      <td>['유통', '전자제품']</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>클라우드</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>옐로모바일</td>\n",
       "      <td>['모바일서비스', '웹서비스', '소프트웨어', '모바일게임', '소셜커머스', ...</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미미박스</td>\n",
       "      <td>['Confluence', 'JIRA', 'Google Apps', 'Slack',...</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>쏘카</td>\n",
       "      <td>['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'O2O...</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>포도트리</td>\n",
       "      <td>['Confluence', 'Google Apps', 'Slack', 'Trello...</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>홈어플라이언스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>데일리금융그룹</td>\n",
       "      <td>['IT서비스', '금융', '핀테크', 'Fintech', 'finance', '...</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>클룩</td>\n",
       "      <td>['웹서비스', '마케팅', '플랫폼', '스타트업', '여행', '액티비티']</td>\n",
       "      <td>에너지</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>자율주행차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>야놀자</td>\n",
       "      <td>['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'IT서...</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                     origianl_label  \\\n",
       "0       쿠팡  ['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'e-c...   \n",
       "1    서울반도체                                            ['제조업']   \n",
       "2   롯데하이마트                                     ['유통', '전자제품']   \n",
       "3    옐로모바일  ['모바일서비스', '웹서비스', '소프트웨어', '모바일게임', '소셜커머스', ...   \n",
       "4     미미박스  ['Confluence', 'JIRA', 'Google Apps', 'Slack',...   \n",
       "5       쏘카  ['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'O2O...   \n",
       "6     포도트리  ['Confluence', 'Google Apps', 'Slack', 'Trello...   \n",
       "7  데일리금융그룹  ['IT서비스', '금융', '핀테크', 'Fintech', 'finance', '...   \n",
       "8       클룩       ['웹서비스', '마케팅', '플랫폼', '스타트업', '여행', '액티비티']   \n",
       "9      야놀자  ['모바일서비스', '웹서비스', '마케팅', '소프트웨어', '데이터', 'IT서...   \n",
       "\n",
       "  predicted_label1 predicted_label2 predicted_label3  \n",
       "0        O2O/지식서비스         스마트미디어기기       디지털콘텐츠·디자인  \n",
       "1            LED/광          홈어플라이언스             전자부품  \n",
       "2        O2O/지식서비스          홈어플라이언스             클라우드  \n",
       "3             블록체인       디지털콘텐츠·디자인        O2O/지식서비스  \n",
       "4        O2O/지식서비스       디지털콘텐츠·디자인          AI/빅데이터  \n",
       "5             블록체인          AI/빅데이터        O2O/지식서비스  \n",
       "6        O2O/지식서비스       디지털콘텐츠·디자인          홈어플라이언스  \n",
       "7              핀테크             블록체인        O2O/지식서비스  \n",
       "8              에너지        O2O/지식서비스            자율주행차  \n",
       "9        O2O/지식서비스         스마트미디어기기          AI/빅데이터  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification for TIPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_list = pd.read_csv('C:/Users/hangy/Desktop/topics/data/tips_list.csv',encoding='utf-8-sig',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_news = pd.read_excel('C:/Users/hangy/Desktop/topics/data/tips_list_news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tips_list['ceo']\n",
    "del tips_list['date']\n",
    "del tips_list['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>doc</th>\n",
       "      <th>information</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(주)시옷플랫폼</td>\n",
       "      <td>시옷플랫폼은 여러 종류의 영상센서(열영상센서와 이미지센서)와 MEMS 센서를 결합한...</td>\n",
       "      <td>{'사업분야': 'IoT(사물인터넷)', '설립일': '2010. 8. 8', '대...</td>\n",
       "      <td>IoT(사물인터넷)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>큐키</td>\n",
       "      <td>스마트폰을 사용하게 되면서 겪는 문제는 문자입력에 오타 입력이 많다는 점을 착안하여...</td>\n",
       "      <td>{'사업분야': '모바일IT', '설립일': '2013.07.15', '대표자': ...</td>\n",
       "      <td>모바일IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(주)위브랩</td>\n",
       "      <td>위브랩은 다음(Daum) 초창기부터 검색엔진 개발과 관리를 이끌어온 핵심인력들로 구...</td>\n",
       "      <td>{'사업분야': 'IT S/W', '설립일': '2013.08.06', '대표자':...</td>\n",
       "      <td>IT S/W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>엔트리움(주)</td>\n",
       "      <td>현재 디스플레이용 비등방성 전도성 집착필름(ACF)에 사용되는 도전성 입자는(약 2...</td>\n",
       "      <td>{'사업분야': '기계·소재', '설립일': '2013.02.01', '대표자': ...</td>\n",
       "      <td>기계·소재</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>프라센</td>\n",
       "      <td>프라센은 KAIST 기계공학, 전자공학 연구원 및 전 삼성전자 소프트웨어 엔지니어가...</td>\n",
       "      <td>{'사업분야': '바이오의료', '설립일': '2010.03.08', '대표자': ...</td>\n",
       "      <td>바이오의료</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    company                                                doc  \\\n",
       "0  (주)시옷플랫폼  시옷플랫폼은 여러 종류의 영상센서(열영상센서와 이미지센서)와 MEMS 센서를 결합한...   \n",
       "1        큐키  스마트폰을 사용하게 되면서 겪는 문제는 문자입력에 오타 입력이 많다는 점을 착안하여...   \n",
       "2    (주)위브랩  위브랩은 다음(Daum) 초창기부터 검색엔진 개발과 관리를 이끌어온 핵심인력들로 구...   \n",
       "3   엔트리움(주)  현재 디스플레이용 비등방성 전도성 집착필름(ACF)에 사용되는 도전성 입자는(약 2...   \n",
       "4       프라센  프라센은 KAIST 기계공학, 전자공학 연구원 및 전 삼성전자 소프트웨어 엔지니어가...   \n",
       "\n",
       "                                         information    category  \n",
       "0  {'사업분야': 'IoT(사물인터넷)', '설립일': '2010. 8. 8', '대...  IoT(사물인터넷)  \n",
       "1  {'사업분야': '모바일IT', '설립일': '2013.07.15', '대표자': ...       모바일IT  \n",
       "2  {'사업분야': 'IT S/W', '설립일': '2013.08.06', '대표자':...      IT S/W  \n",
       "3  {'사업분야': '기계·소재', '설립일': '2013.02.01', '대표자': ...       기계·소재  \n",
       "4  {'사업분야': '바이오의료', '설립일': '2010.03.08', '대표자': ...       바이오의료  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_idx = []\n",
    "for i in range(len(tips_news)) : \n",
    "    comapny = tips_news['company'][i]\n",
    "    company_name = comapny.replace(\"(주)\",\"\")\n",
    "    if company_name in tips_news['title'][i] :\n",
    "        filter_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_news = tips_news[tips_news.index.isin(filter_idx)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_news = filter_news.drop_duplicates(subset='company', keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_tips_list = pd.merge(tips_list, filter_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>doc</th>\n",
       "      <th>information</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>press</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>큐키</td>\n",
       "      <td>스마트폰을 사용하게 되면서 겪는 문제는 문자입력에 오타 입력이 많다는 점을 착안하여...</td>\n",
       "      <td>{'사업분야': '모바일IT', '설립일': '2013.07.15', '대표자': ...</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>김민철 큐키 대표</td>\n",
       "      <td>한경비즈니스</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2016-04-26 18:03</td>\n",
       "      <td>대한민국 스타트업 100인김민철 큐키 대표  스마트폰 오타 수정 어렵지 않아요 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(주)위브랩</td>\n",
       "      <td>위브랩은 다음(Daum) 초창기부터 검색엔진 개발과 관리를 이끌어온 핵심인력들로 구...</td>\n",
       "      <td>{'사업분야': 'IT S/W', '설립일': '2013.08.06', '대표자':...</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>[2020유니콘]&lt;19&gt;위브랩</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2016-10-05 17:00</td>\n",
       "      <td>0302016100586215520161004141829624000299201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>엔트리움(주)</td>\n",
       "      <td>현재 디스플레이용 비등방성 전도성 집착필름(ACF)에 사용되는 도전성 입자는(약 2...</td>\n",
       "      <td>{'사업분야': '기계·소재', '설립일': '2013.02.01', '대표자': ...</td>\n",
       "      <td>기계·소재</td>\n",
       "      <td>[나노 강소기업을 가다]&lt;4&gt;엔트리움, 신개념 전자파 차폐 소재 본격 양산…올해 '...</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-02-19 17:02</td>\n",
       "      <td>엔트리움이 하이닉스와 협업해 개발한 스프레이 방식 전자파 차폐 소재를 올해 본격 양...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>비트패킹컴퍼니</td>\n",
       "      <td>비트패킹컴퍼니는 음악을 무료로 들을 수 있는 모바일 전용 스트리밍 라디오 앱, ‘비...</td>\n",
       "      <td>{'사업분야': '모바일IT', '설립일': '2013.4.24', '대표자': '...</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>비트패킹컴퍼니 결국 서비스 종료...\"추가 투자 유치 실패로 서비스 지속 불가\"</td>\n",
       "      <td>조선비즈</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2016-12-01 11:40</td>\n",
       "      <td>음악 스트리밍 업체 비트패킹컴퍼니가 지난달 30일 서비스를 종료했다 1년 넘게 추가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(주)리브스메드</td>\n",
       "      <td>기존의 복강경 수술 기구는 관절이 없는 일자형으로서, 수술 동작의 수행이 쉽지 않고...</td>\n",
       "      <td>{'사업분야': '바이오의료', '설립일': '2011.6.20', '대표자': '...</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>리브스메드, ‘복강경 기구 국산화’ 복지부 장관상 수상</td>\n",
       "      <td>헤럴드경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2018-11-06 15:01</td>\n",
       "      <td>0162018110620181106000903020181106150106592...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    company                                                doc  \\\n",
       "0        큐키  스마트폰을 사용하게 되면서 겪는 문제는 문자입력에 오타 입력이 많다는 점을 착안하여...   \n",
       "1    (주)위브랩  위브랩은 다음(Daum) 초창기부터 검색엔진 개발과 관리를 이끌어온 핵심인력들로 구...   \n",
       "2   엔트리움(주)  현재 디스플레이용 비등방성 전도성 집착필름(ACF)에 사용되는 도전성 입자는(약 2...   \n",
       "3   비트패킹컴퍼니  비트패킹컴퍼니는 음악을 무료로 들을 수 있는 모바일 전용 스트리밍 라디오 앱, ‘비...   \n",
       "4  (주)리브스메드  기존의 복강경 수술 기구는 관절이 없는 일자형으로서, 수술 동작의 수행이 쉽지 않고...   \n",
       "\n",
       "                                         information category  \\\n",
       "0  {'사업분야': '모바일IT', '설립일': '2013.07.15', '대표자': ...    모바일IT   \n",
       "1  {'사업분야': 'IT S/W', '설립일': '2013.08.06', '대표자':...   IT S/W   \n",
       "2  {'사업분야': '기계·소재', '설립일': '2013.02.01', '대표자': ...    기계·소재   \n",
       "3  {'사업분야': '모바일IT', '설립일': '2013.4.24', '대표자': '...    모바일IT   \n",
       "4  {'사업분야': '바이오의료', '설립일': '2011.6.20', '대표자': '...    바이오의료   \n",
       "\n",
       "                                               title   press  \\\n",
       "0                                          김민철 큐키 대표  한경비즈니스   \n",
       "1                                   [2020유니콘]<19>위브랩    전자신문   \n",
       "2  [나노 강소기업을 가다]<4>엔트리움, 신개념 전자파 차폐 소재 본격 양산…올해 '...    전자신문   \n",
       "3       비트패킹컴퍼니 결국 서비스 종료...\"추가 투자 유치 실패로 서비스 지속 불가\"    조선비즈   \n",
       "4                     리브스메드, ‘복강경 기구 국산화’ 복지부 장관상 수상   헤럴드경제   \n",
       "\n",
       "                                                link              date  \\\n",
       "0  https://news.naver.com/main/read.nhn?mode=LSD&...  2016-04-26 18:03   \n",
       "1  https://news.naver.com/main/read.nhn?mode=LSD&...  2016-10-05 17:00   \n",
       "2  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-02-19 17:02   \n",
       "3  https://news.naver.com/main/read.nhn?mode=LSD&...  2016-12-01 11:40   \n",
       "4  https://news.naver.com/main/read.nhn?mode=LSD&...  2018-11-06 15:01   \n",
       "\n",
       "                                             content  \n",
       "0    대한민국 스타트업 100인김민철 큐키 대표  스마트폰 오타 수정 어렵지 않아요 ...  \n",
       "1     0302016100586215520161004141829624000299201...  \n",
       "2  엔트리움이 하이닉스와 협업해 개발한 스프레이 방식 전자파 차폐 소재를 올해 본격 양...  \n",
       "3  음악 스트리밍 업체 비트패킹컴퍼니가 지난달 30일 서비스를 종료했다 1년 넘게 추가...  \n",
       "4     0162018110620181106000903020181106150106592...  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tips_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_tips_list.to_excel('C:/Users/hangy/Desktop/sub_tips_list.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 32996 from 303 sents. mem=2.337 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=94241, mem=2.374 Gb\n",
      "[Noun Extractor] batch prediction was completed for 13233 words\n",
      "[Noun Extractor] checked compounds. discovered 3065 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 7510 -> 7110\n",
      "[Noun Extractor] postprocessing ignore_features : 7110 -> 7078\n",
      "[Noun Extractor] postprocessing ignore_NJ : 7078 -> 7069\n",
      "[Noun Extractor] 7069 nouns (3065 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=2.384 Gb                    \n",
      "[Noun Extractor] 73.83 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "tips_document = []\n",
    "for i in range(len(sub_tips_list)):\n",
    "    tips_document.append(str(sub_tips_list['doc'][i])+str(sub_tips_list['content'][i]))\n",
    "    \n",
    "for i in range(len(tips_document)):\n",
    "    tips_document[i] = re.sub('[\\n\\uf000\\x0c()/▪,.ㆍ\\[\\]0-9\\'●※\\\"▷▶:\\};&\\-><%|🌟✔️]',' ', tips_document[i])\n",
    "    \n",
    "tips_data = pd.DataFrame({'company':sub_tips_list['company'],'category': sub_tips_list['category'],\n",
    "                          'document':tips_document,'lable':['']*len(tips_document)})\n",
    "\n",
    "tips_corpus = noun_corpus(tips_data['document'])\n",
    "tips_docs = stopwords(stops, tips_corpus)\n",
    "tips_data['token'] = tips_docs\n",
    "\n",
    "stops = list(ko_stopwords['stopwords'])\n",
    "tips_docs = stopwords(stops, tips_docs)\n",
    "tips_data['token'] = tips_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_tokens = [ t for d in tips_data['token'] for t in d]\n",
    "tips_text = nltk.Text(tips_tokens, name='NMSC')\n",
    "tips_fdist = tips_text.vocab()\n",
    "\n",
    "tips_fdist = pd.DataFrame.from_dict(tips_fdist, orient='index')\n",
    "tips_fdist.columns = ['frequency']\n",
    "tips_fdist['term'] = list(tips_fdist.index)\n",
    "tips_fdist = tips_fdist.reset_index(drop=True)\n",
    "tips_fdist = tips_fdist.sort_values([\"frequency\"], ascending=[False])\n",
    "tips_fdist = tips_fdist.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zif'slow \n",
    "zif_list = pd.DataFrame(tips_fdist[(tips_fdist['frequency'] >200) | (tips_fdist['frequency'] <= 2)]['term'])\n",
    "zif_list.columns = ['stopwords']\n",
    "zif_list = zif_list.reset_index(drop=True)\n",
    "\n",
    "stops = list(zif_list['stopwords'])\n",
    "tips_docs2 = stopwords(stops, tips_docs)\n",
    "tips_data['token'] = tips_docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_len =[]\n",
    "for i in range(len(tips_data)):\n",
    "    token_len.append(len(tips_data['token'][i]))\n",
    "tips_data['token_len'] = token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tips_docs = [TaggedDocument(d, c) for d, c in tips_data[['token', 'lable']].values]\n",
    "X_tips = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_tips_docs]\n",
    "y_tips = [doc.tags for doc in tagged_tips_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_company_pred = mlp_clf.predict(X_tips)\n",
    "y_company_prob = mlp_clf.predict_proba(X_tips)\n",
    "L = np.argsort(-y_company_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {mlp_clf.classes_[i]: i for i in range(len(mlp_clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "\n",
    "for i in range(len(y_company_prob)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    thrid = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(thrid)]])\n",
    "    dd.append({'company':tips_data['company'][i],'origianl_label':tips_data['category'][i],\n",
    "               'predicted_label1':label[0], 'predicted_label2': label[1], 'predicted_label3': label[2]})\n",
    "\n",
    "#company_pred_df = pd.DataFrame(dd,columns = ['company','origianl_label','predicted_label1','predicted_label2','predicted_label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_company_pred = mlp_clf2.predict(X_tips)\n",
    "y_company_prob = mlp_clf2.predict_proba(X_tips)\n",
    "L = np.argsort(-y_company_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {mlp_clf2.classes_[i]: i for i in range(len(mlp_clf2.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result = []\n",
    "score = []\n",
    "for i in range(len(X_tips)):\n",
    "    tt = category_dic[dd[i]['predicted_label1']] + category_dic[dd[i]['predicted_label2']]\n",
    "    tt = [x for x in tt if x in key_list]\n",
    "    ttt = list({ your_key: class_dic[your_key] for your_key in tt }.values())\n",
    "    tttt = [x for x in L[i] if x in ttt]\n",
    "    first = tttt[0]\n",
    "    second = tttt[1]\n",
    "    third = tttt[2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(third)]])\n",
    "    if y_test2[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    #second_result.append({'y_test':y_test2[i], 'first':label[0], 'second': label[1],'third': label[2]})\n",
    "    second_result.append({'company':tips_data['company'][i],'origianl_label':tips_data['category'][i],\n",
    "           'big_predict1' :dd[i]['predicted_label1'],'big_predict2' :dd[i]['predicted_label2'],\n",
    "           'predicted_label1':label[0], 'predicted_label2': label[1], 'predicted_label3': label[2]})\n",
    "acc = sum(score)/len(second_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>big_predict1</th>\n",
       "      <th>big_predict2</th>\n",
       "      <th>company</th>\n",
       "      <th>origianl_label</th>\n",
       "      <th>predicted_label1</th>\n",
       "      <th>predicted_label2</th>\n",
       "      <th>predicted_label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>큐키</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>텍스트 기반 시각 데이터 이해 및 검색 서비스</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>바이오 시큐리티 금융서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>(주)위브랩</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>영상처리 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>지능형반도체</td>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>엔트리움(주)</td>\n",
       "      <td>기계·소재</td>\n",
       "      <td>SoC 부품</td>\n",
       "      <td>전력반도체소자</td>\n",
       "      <td>초저전압 고신뢰성 뉴로모픽 엔진탑재 마이크로컨트롤러</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>비트패킹컴퍼니</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "      <td>음성인식 SW</td>\n",
       "      <td>데이터 보안 및 비식별화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>LED/광</td>\n",
       "      <td>(주)리브스메드</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>치료, 수술용 기구 및 기기</td>\n",
       "      <td>재활 및 보조기기</td>\n",
       "      <td>신광원기반 일반조명시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>(주)유저해빗</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>텍스트 기반 시각 데이터 이해 및 검색 서비스</td>\n",
       "      <td>음성인식 SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>(주)크레스프리</td>\n",
       "      <td>IoT(사물인터넷)</td>\n",
       "      <td>공공 시설물 디자인 기술</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>(주)다노</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "      <td>데이터 보안 및 비식별화</td>\n",
       "      <td>음성인식 SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>(주)라이클</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>블록체인 기반 온라인 쇼핑몰</td>\n",
       "      <td>O2O 서비스 중개 플랫폼</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>(주)브레이브팝스컴퍼니</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>에듀테크 콘텐츠</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>(주)드라마앤컴퍼니</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>블록체인 기반 온라인 쇼핑몰</td>\n",
       "      <td>자산관리 서비스</td>\n",
       "      <td>인슈어테크 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>사물인터넷</td>\n",
       "      <td>(주)오비이랩</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>가정용 의료기기 및 플랫폼</td>\n",
       "      <td>유전체분석(NGS) 플랫폼 비즈니스</td>\n",
       "      <td>스마트 현장 진단기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>사물인터넷</td>\n",
       "      <td>(주)쿠쿠닥스</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>데이터 보안 및 비식별화</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>IoT 스마트 뷰티</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>에너지</td>\n",
       "      <td>(주)파킹스퀘어</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>스마트홈/비즈니스</td>\n",
       "      <td>스마트 미러</td>\n",
       "      <td>스마트 비서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>(주)망고플레이트</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>블록체인 기반 u-wellness 서비스</td>\n",
       "      <td>스마트 헬스케어</td>\n",
       "      <td>블록체인 기반 소유자 이력관리 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>지능형반도체</td>\n",
       "      <td>(주)스트라티오코리아</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>반도체 센서</td>\n",
       "      <td>휴대용 생체인증기기·시스템</td>\n",
       "      <td>생활약자용 웨어러블 디바이스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>클라우드</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>(주)조이코퍼레이션</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>레그테크 서비스</td>\n",
       "      <td>인지 컴퓨팅</td>\n",
       "      <td>자산관리 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>다이닝코드</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>공공서비스 특화 챗봇 시스템</td>\n",
       "      <td>산업기술 지식화 서비스</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>첨단소재</td>\n",
       "      <td>화학및섬유소재</td>\n",
       "      <td>(주)유비파이</td>\n",
       "      <td>기계·소재</td>\n",
       "      <td>비탄소계 저차원 소재</td>\n",
       "      <td>기타섬유</td>\n",
       "      <td>기계·구조세라믹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>해보라(주)</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>IT접목 가방</td>\n",
       "      <td>스마트 시계/밴드</td>\n",
       "      <td>텍스트 기반 시각 데이터 이해 및 검색 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>(주)와이드벤티지</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>모바일 콘텐츠</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>실감방송 콘텐츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>엔트리교육연구소</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>에듀테크 콘텐츠</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>개인맞춤형 스마트러닝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>비트파인더</td>\n",
       "      <td>IoT(사물인터넷)</td>\n",
       "      <td>고효율 난방기기</td>\n",
       "      <td>현관 에어 케어 시스템</td>\n",
       "      <td>IT접목 가방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>사물인터넷</td>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>(주)스파코사</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>IoT 스마트 완구 및 교구</td>\n",
       "      <td>콘텐츠 결합 스마트 미용가전</td>\n",
       "      <td>기능성 스마트 침대</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>지능형반도체</td>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>(주)테그웨이</td>\n",
       "      <td>전기전자</td>\n",
       "      <td>산업용 고신뢰/저전력 네트워킹</td>\n",
       "      <td>ICT‧센서‧조명기술 융합 긴급 재난안전 장치</td>\n",
       "      <td>광학부품 및 기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>브레인커머스(주)</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>데이터 3D 변환 시각화 도구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>첨단소재</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>(주)요쿠스</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>텍스트 기반 시각 데이터 이해 및 검색 서비스</td>\n",
       "      <td>데이터 3D 변환 시각화 도구</td>\n",
       "      <td>공공서비스 특화 챗봇 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>클라우드</td>\n",
       "      <td>(주)레클</td>\n",
       "      <td>모바일IT</td>\n",
       "      <td>인메모리 컴퓨팅</td>\n",
       "      <td>클라우드 기반 SW</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>(주)시어스랩</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>공공서비스 특화 챗봇 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>핀테크</td>\n",
       "      <td>정보보안</td>\n",
       "      <td>(주)위버플</td>\n",
       "      <td>IT S/W</td>\n",
       "      <td>핀테크 빅데이터 분석 및 활용서비스</td>\n",
       "      <td>인슈어테크 서비스</td>\n",
       "      <td>자산관리 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>블록오디세이</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>블록체인 기반 온라인 쇼핑몰</td>\n",
       "      <td>블록체인 기반 소유자 이력관리 시스템</td>\n",
       "      <td>블록체인 기반 소액 기부/후원 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>홈어플라이언스</td>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>메텔</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>에어가전</td>\n",
       "      <td>스마트키친 디바이스</td>\n",
       "      <td>융·복합형 정수기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>산타</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>체험형 전시 콘텐츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>집토스</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>블록체인 기반 온라인 쇼핑몰</td>\n",
       "      <td>핀테크 빅데이터 분석 및 활용서비스</td>\n",
       "      <td>크라우드펀딩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>홀짝</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>데이터 보안 및 비식별화</td>\n",
       "      <td>CG</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>디스플레이</td>\n",
       "      <td>아톰앤비트</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>디지털 사이니지용 디스플레이</td>\n",
       "      <td>가상/증강현실 콘텐츠</td>\n",
       "      <td>Head Mounted Display(HMD) 용 모니터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>부루구루</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>스마트 제조 CPS</td>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>등각냉각 채널 적용 3D프린팅 금형 설계 및 제작시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>식품</td>\n",
       "      <td>에너지</td>\n",
       "      <td>이너보틀</td>\n",
       "      <td>기타</td>\n",
       "      <td>기능성 식품 패키징 시스템</td>\n",
       "      <td>태양광 발전</td>\n",
       "      <td>식품 장기 보관 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>전자부품</td>\n",
       "      <td>테코플러스</td>\n",
       "      <td>화학</td>\n",
       "      <td>중소제조기업용 협동로봇</td>\n",
       "      <td>3D프린팅 기반 친환경자동차용 전장부품</td>\n",
       "      <td>등각냉각 채널 적용 3D프린팅 금형 설계 및 제작시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>큐라움</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>인공지능 기반 헬스케어 데이터 분석</td>\n",
       "      <td>실시간 건강관리 시스템</td>\n",
       "      <td>가정용 의료기기 및 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>파코웨어</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>스마트 완구</td>\n",
       "      <td>3D프린팅 제품설계 디자인</td>\n",
       "      <td>에듀테크 콘텐츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>정보보안</td>\n",
       "      <td>소테리아</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>데이터 보안 및 비식별화</td>\n",
       "      <td>헬스케어/의료 보안</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>스마트미디어기기</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>삼십구도씨</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>AR/VR 기반 운동기구</td>\n",
       "      <td>디지털 셋톱박스</td>\n",
       "      <td>IT접목 가방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>스마트팩토리</td>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>팡세</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>재활 및 보조기기</td>\n",
       "      <td>친환경 고효율 선박기자재</td>\n",
       "      <td>생체신호 측정진단기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>한국축산데이터</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>인공지능 기반 헬스케어 데이터 분석</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>유전체분석(NGS) 플랫폼 비즈니스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>지니너스</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>실감방송 콘텐츠</td>\n",
       "      <td>의료정보관리 플랫폼</td>\n",
       "      <td>유전체분석(NGS) 플랫폼 비즈니스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>O2O/지식서비스</td>\n",
       "      <td>더맘마</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>산업기술 지식화 서비스</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>생산설비 유지관리 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>핀테크</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>쉐어트리츠</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>자산관리 서비스</td>\n",
       "      <td>국내외 간편 송금 및 결제</td>\n",
       "      <td>공공서비스 특화 챗봇 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>알키온</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>블록체인 기반 온라인 쇼핑몰</td>\n",
       "      <td>블록체인 기반 공유경제 서비스</td>\n",
       "      <td>블록체인 기반 중고거래 융합 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>화학및섬유소재</td>\n",
       "      <td>디스플레이</td>\n",
       "      <td>퓨리파이테크노</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>기타섬유</td>\n",
       "      <td>미세전자기계 시스템</td>\n",
       "      <td>금속 및 세라믹소재</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>블록체인</td>\n",
       "      <td>일반기계</td>\n",
       "      <td>업스테어스</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>블록체인 기반 중고거래 융합 플랫폼</td>\n",
       "      <td>블록체인 기반 공유경제 서비스</td>\n",
       "      <td>블록체인 기반 u-wellness 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>핀테크</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>모바일퉁</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>국내외 간편 송금 및 결제</td>\n",
       "      <td>블록체인 기반 소액 기부/후원 플랫폼</td>\n",
       "      <td>블록체인 기반 소유자 이력관리 시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>사물인터넷</td>\n",
       "      <td>지능형반도체</td>\n",
       "      <td>그린존시큐리티</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>능동형 IoT 디바이스</td>\n",
       "      <td>IoT 스마트 완구 및 교구</td>\n",
       "      <td>반도체 센서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>스페이스워크</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>빅데이터 분석 및 시각화 플랫폼</td>\n",
       "      <td>인공지능 플랫폼</td>\n",
       "      <td>음성인식 SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>첨단소재</td>\n",
       "      <td>바이오트코리아</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>스마트 현장 진단기기</td>\n",
       "      <td>보건·의료용 기능성 소재</td>\n",
       "      <td>가정용 의료기기 및 플랫폼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>핀테크</td>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>발트루스트</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>핀테크 빅데이터 분석 및 활용서비스</td>\n",
       "      <td>크라우드펀딩</td>\n",
       "      <td>모바일 핀테크 보안</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>디지털콘텐츠·디자인</td>\n",
       "      <td>지능형로봇</td>\n",
       "      <td>멘토릿</td>\n",
       "      <td>정보통신</td>\n",
       "      <td>에듀테크 콘텐츠</td>\n",
       "      <td>체험형 전시 콘텐츠</td>\n",
       "      <td>소셜 로봇 플랫폼 및 서비스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>블록체인</td>\n",
       "      <td>골골송작곡가</td>\n",
       "      <td>전기전자</td>\n",
       "      <td>가정용 의료기기 및 플랫폼</td>\n",
       "      <td>블록체인 기반 u-wellness 서비스</td>\n",
       "      <td>스마트 안티 폴루션 추천시스템</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>화학및섬유소재</td>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>메디허브</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>면역억제제</td>\n",
       "      <td>천연 기능성 화장품</td>\n",
       "      <td>스마트 헬스케어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>의료서비스·기기</td>\n",
       "      <td>AI/빅데이터</td>\n",
       "      <td>브레인기어</td>\n",
       "      <td>바이오의료</td>\n",
       "      <td>치료, 수술용 기구 및 기기</td>\n",
       "      <td>스마트 현장 진단기기</td>\n",
       "      <td>면역억제제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    big_predict1 big_predict2       company origianl_label  \\\n",
       "0        AI/빅데이터    O2O/지식서비스            큐키          모바일IT   \n",
       "1        AI/빅데이터   디지털콘텐츠·디자인        (주)위브랩         IT S/W   \n",
       "2         지능형반도체       스마트팩토리       엔트리움(주)          기계·소재   \n",
       "3      O2O/지식서비스      AI/빅데이터       비트패킹컴퍼니          모바일IT   \n",
       "4       의료서비스·기기        LED/광      (주)리브스메드          바이오의료   \n",
       "5        AI/빅데이터          핀테크       (주)유저해빗          모바일IT   \n",
       "6      O2O/지식서비스   디지털콘텐츠·디자인      (주)크레스프리     IoT(사물인터넷)   \n",
       "7        AI/빅데이터    O2O/지식서비스         (주)다노         IT S/W   \n",
       "8           블록체인    O2O/지식서비스        (주)라이클          모바일IT   \n",
       "9     디지털콘텐츠·디자인    O2O/지식서비스  (주)브레이브팝스컴퍼니         IT S/W   \n",
       "10          블록체인          핀테크    (주)드라마앤컴퍼니          모바일IT   \n",
       "11      의료서비스·기기        사물인터넷       (주)오비이랩          바이오의료   \n",
       "12       AI/빅데이터        사물인터넷       (주)쿠쿠닥스          모바일IT   \n",
       "13       홈어플라이언스          에너지      (주)파킹스퀘어          모바일IT   \n",
       "14      의료서비스·기기         블록체인     (주)망고플레이트          모바일IT   \n",
       "15      스마트미디어기기       지능형반도체   (주)스트라티오코리아          바이오의료   \n",
       "16          클라우드          핀테크    (주)조이코퍼레이션         IT S/W   \n",
       "17     O2O/지식서비스      AI/빅데이터         다이닝코드         IT S/W   \n",
       "18          첨단소재      화학및섬유소재       (주)유비파이          기계·소재   \n",
       "19       AI/빅데이터     스마트미디어기기        해보라(주)          모바일IT   \n",
       "20    디지털콘텐츠·디자인     스마트미디어기기     (주)와이드벤티지          모바일IT   \n",
       "21    디지털콘텐츠·디자인    O2O/지식서비스      엔트리교육연구소         IT S/W   \n",
       "22      스마트미디어기기      홈어플라이언스         비트파인더     IoT(사물인터넷)   \n",
       "23         사물인터넷      홈어플라이언스       (주)스파코사          모바일IT   \n",
       "24        지능형반도체       스마트팩토리       (주)테그웨이           전기전자   \n",
       "25       AI/빅데이터          핀테크     브레인커머스(주)          모바일IT   \n",
       "26          첨단소재      AI/빅데이터        (주)요쿠스         IT S/W   \n",
       "27       AI/빅데이터         클라우드         (주)레클          모바일IT   \n",
       "28       AI/빅데이터    O2O/지식서비스       (주)시어스랩         IT S/W   \n",
       "29           핀테크         정보보안        (주)위버플         IT S/W   \n",
       "..           ...          ...           ...            ...   \n",
       "273         블록체인    O2O/지식서비스        블록오디세이           정보통신   \n",
       "274      홈어플라이언스     의료서비스·기기            메텔           정보통신   \n",
       "275      AI/빅데이터   디지털콘텐츠·디자인            산타           정보통신   \n",
       "276         블록체인          핀테크           집토스           정보통신   \n",
       "277      AI/빅데이터   디지털콘텐츠·디자인            홀짝           정보통신   \n",
       "278   디지털콘텐츠·디자인        디스플레이         아톰앤비트           정보통신   \n",
       "279       스마트팩토리    O2O/지식서비스          부루구루           정보통신   \n",
       "280           식품          에너지          이너보틀             기타   \n",
       "281       스마트팩토리         전자부품         테코플러스             화학   \n",
       "282     의료서비스·기기      AI/빅데이터           큐라움          바이오의료   \n",
       "283   디지털콘텐츠·디자인       스마트팩토리          파코웨어           정보통신   \n",
       "284      AI/빅데이터         정보보안          소테리아           정보통신   \n",
       "285     스마트미디어기기      AI/빅데이터         삼십구도씨           정보통신   \n",
       "286       스마트팩토리     의료서비스·기기            팡세          바이오의료   \n",
       "287      AI/빅데이터     의료서비스·기기       한국축산데이터          바이오의료   \n",
       "288   디지털콘텐츠·디자인     의료서비스·기기          지니너스          바이오의료   \n",
       "289      AI/빅데이터    O2O/지식서비스           더맘마           정보통신   \n",
       "290          핀테크      AI/빅데이터         쉐어트리츠           정보통신   \n",
       "291         블록체인      AI/빅데이터           알키온           정보통신   \n",
       "292      화학및섬유소재        디스플레이       퓨리파이테크노           정보통신   \n",
       "293         블록체인         일반기계         업스테어스           정보통신   \n",
       "294          핀테크         블록체인          모바일퉁           정보통신   \n",
       "295        사물인터넷       지능형반도체       그린존시큐리티           정보통신   \n",
       "296      AI/빅데이터          핀테크        스페이스워크           정보통신   \n",
       "297     의료서비스·기기         첨단소재       바이오트코리아          바이오의료   \n",
       "298          핀테크   디지털콘텐츠·디자인         발트루스트           정보통신   \n",
       "299   디지털콘텐츠·디자인        지능형로봇           멘토릿           정보통신   \n",
       "300     의료서비스·기기         블록체인        골골송작곡가           전기전자   \n",
       "301      화학및섬유소재     의료서비스·기기          메디허브          바이오의료   \n",
       "302     의료서비스·기기      AI/빅데이터         브레인기어          바이오의료   \n",
       "\n",
       "              predicted_label1           predicted_label2  \\\n",
       "0    텍스트 기반 시각 데이터 이해 및 검색 서비스                   인공지능 플랫폼   \n",
       "1                     인공지능 플랫폼          빅데이터 분석 및 시각화 플랫폼   \n",
       "2                       SoC 부품                    전력반도체소자   \n",
       "3                  개인맞춤형 스마트러닝                    음성인식 SW   \n",
       "4              치료, 수술용 기구 및 기기                  재활 및 보조기기   \n",
       "5            빅데이터 분석 및 시각화 플랫폼  텍스트 기반 시각 데이터 이해 및 검색 서비스   \n",
       "6                공공 시설물 디자인 기술                     스마트 완구   \n",
       "7                  개인맞춤형 스마트러닝              데이터 보안 및 비식별화   \n",
       "8              블록체인 기반 온라인 쇼핑몰             O2O 서비스 중개 플랫폼   \n",
       "9                     에듀테크 콘텐츠                     스마트 완구   \n",
       "10             블록체인 기반 온라인 쇼핑몰                   자산관리 서비스   \n",
       "11              가정용 의료기기 및 플랫폼        유전체분석(NGS) 플랫폼 비즈니스   \n",
       "12               데이터 보안 및 비식별화          빅데이터 분석 및 시각화 플랫폼   \n",
       "13                   스마트홈/비즈니스                     스마트 미러   \n",
       "14      블록체인 기반 u-wellness 서비스                   스마트 헬스케어   \n",
       "15                      반도체 센서             휴대용 생체인증기기·시스템   \n",
       "16                    레그테크 서비스                     인지 컴퓨팅   \n",
       "17             공공서비스 특화 챗봇 시스템               산업기술 지식화 서비스   \n",
       "18                 비탄소계 저차원 소재                       기타섬유   \n",
       "19                     IT접목 가방                  스마트 시계/밴드   \n",
       "20                     모바일 콘텐츠                     스마트 완구   \n",
       "21                    에듀테크 콘텐츠                     스마트 완구   \n",
       "22                    고효율 난방기기               현관 에어 케어 시스템   \n",
       "23             IoT 스마트 완구 및 교구            콘텐츠 결합 스마트 미용가전   \n",
       "24            산업용 고신뢰/저전력 네트워킹  ICT‧센서‧조명기술 융합 긴급 재난안전 장치   \n",
       "25           빅데이터 분석 및 시각화 플랫폼                   인공지능 플랫폼   \n",
       "26   텍스트 기반 시각 데이터 이해 및 검색 서비스           데이터 3D 변환 시각화 도구   \n",
       "27                    인메모리 컴퓨팅                 클라우드 기반 SW   \n",
       "28           빅데이터 분석 및 시각화 플랫폼                   인공지능 플랫폼   \n",
       "29         핀테크 빅데이터 분석 및 활용서비스                  인슈어테크 서비스   \n",
       "..                         ...                        ...   \n",
       "273            블록체인 기반 온라인 쇼핑몰       블록체인 기반 소유자 이력관리 시스템   \n",
       "274                       에어가전                 스마트키친 디바이스   \n",
       "275                     스마트 완구                   인공지능 플랫폼   \n",
       "276            블록체인 기반 온라인 쇼핑몰        핀테크 빅데이터 분석 및 활용서비스   \n",
       "277              데이터 보안 및 비식별화                         CG   \n",
       "278            디지털 사이니지용 디스플레이                가상/증강현실 콘텐츠   \n",
       "279                 스마트 제조 CPS                     스마트팩토리   \n",
       "280             기능성 식품 패키징 시스템                     태양광 발전   \n",
       "281               중소제조기업용 협동로봇      3D프린팅 기반 친환경자동차용 전장부품   \n",
       "282        인공지능 기반 헬스케어 데이터 분석               실시간 건강관리 시스템   \n",
       "283                     스마트 완구             3D프린팅 제품설계 디자인   \n",
       "284          빅데이터 분석 및 시각화 플랫폼              데이터 보안 및 비식별화   \n",
       "285              AR/VR 기반 운동기구                   디지털 셋톱박스   \n",
       "286                  재활 및 보조기기              친환경 고효율 선박기자재   \n",
       "287        인공지능 기반 헬스케어 데이터 분석                   인공지능 플랫폼   \n",
       "288                   실감방송 콘텐츠                 의료정보관리 플랫폼   \n",
       "289               산업기술 지식화 서비스          빅데이터 분석 및 시각화 플랫폼   \n",
       "290                   자산관리 서비스             국내외 간편 송금 및 결제   \n",
       "291            블록체인 기반 온라인 쇼핑몰           블록체인 기반 공유경제 서비스   \n",
       "292                       기타섬유                 미세전자기계 시스템   \n",
       "293        블록체인 기반 중고거래 융합 플랫폼           블록체인 기반 공유경제 서비스   \n",
       "294             국내외 간편 송금 및 결제       블록체인 기반 소액 기부/후원 플랫폼   \n",
       "295               능동형 IoT 디바이스            IoT 스마트 완구 및 교구   \n",
       "296          빅데이터 분석 및 시각화 플랫폼                   인공지능 플랫폼   \n",
       "297                스마트 현장 진단기기              보건·의료용 기능성 소재   \n",
       "298        핀테크 빅데이터 분석 및 활용서비스                     크라우드펀딩   \n",
       "299                   에듀테크 콘텐츠                 체험형 전시 콘텐츠   \n",
       "300             가정용 의료기기 및 플랫폼     블록체인 기반 u-wellness 서비스   \n",
       "301                      면역억제제                 천연 기능성 화장품   \n",
       "302            치료, 수술용 기구 및 기기                스마트 현장 진단기기   \n",
       "\n",
       "                    predicted_label3  \n",
       "0                     바이오 시큐리티 금융서비스  \n",
       "1                           영상처리 시스템  \n",
       "2       초저전압 고신뢰성 뉴로모픽 엔진탑재 마이크로컨트롤러  \n",
       "3                      데이터 보안 및 비식별화  \n",
       "4                      신광원기반 일반조명시스템  \n",
       "5                            음성인식 SW  \n",
       "6                        개인맞춤형 스마트러닝  \n",
       "7                            음성인식 SW  \n",
       "8                        개인맞춤형 스마트러닝  \n",
       "9                        개인맞춤형 스마트러닝  \n",
       "10                         인슈어테크 서비스  \n",
       "11                       스마트 현장 진단기기  \n",
       "12                        IoT 스마트 뷰티  \n",
       "13                            스마트 비서  \n",
       "14              블록체인 기반 소유자 이력관리 시스템  \n",
       "15                   생활약자용 웨어러블 디바이스  \n",
       "16                          자산관리 서비스  \n",
       "17                 빅데이터 분석 및 시각화 플랫폼  \n",
       "18                          기계·구조세라믹  \n",
       "19         텍스트 기반 시각 데이터 이해 및 검색 서비스  \n",
       "20                          실감방송 콘텐츠  \n",
       "21                       개인맞춤형 스마트러닝  \n",
       "22                           IT접목 가방  \n",
       "23                        기능성 스마트 침대  \n",
       "24                         광학부품 및 기기  \n",
       "25                  데이터 3D 변환 시각화 도구  \n",
       "26                   공공서비스 특화 챗봇 시스템  \n",
       "27                 빅데이터 분석 및 시각화 플랫폼  \n",
       "28                   공공서비스 특화 챗봇 시스템  \n",
       "29                          자산관리 서비스  \n",
       "..                               ...  \n",
       "273             블록체인 기반 소액 기부/후원 플랫폼  \n",
       "274                        융·복합형 정수기  \n",
       "275                       체험형 전시 콘텐츠  \n",
       "276                           크라우드펀딩  \n",
       "277                빅데이터 분석 및 시각화 플랫폼  \n",
       "278  Head Mounted Display(HMD) 용 모니터  \n",
       "279   등각냉각 채널 적용 3D프린팅 금형 설계 및 제작시스템  \n",
       "280                     식품 장기 보관 시스템  \n",
       "281   등각냉각 채널 적용 3D프린팅 금형 설계 및 제작시스템  \n",
       "282                   가정용 의료기기 및 플랫폼  \n",
       "283                         에듀테크 콘텐츠  \n",
       "284                       헬스케어/의료 보안  \n",
       "285                          IT접목 가방  \n",
       "286                      생체신호 측정진단기기  \n",
       "287              유전체분석(NGS) 플랫폼 비즈니스  \n",
       "288              유전체분석(NGS) 플랫폼 비즈니스  \n",
       "289                    생산설비 유지관리 서비스  \n",
       "290                  공공서비스 특화 챗봇 시스템  \n",
       "291              블록체인 기반 중고거래 융합 플랫폼  \n",
       "292                       금속 및 세라믹소재  \n",
       "293           블록체인 기반 u-wellness 서비스  \n",
       "294             블록체인 기반 소유자 이력관리 시스템  \n",
       "295                           반도체 센서  \n",
       "296                          음성인식 SW  \n",
       "297                   가정용 의료기기 및 플랫폼  \n",
       "298                       모바일 핀테크 보안  \n",
       "299                  소셜 로봇 플랫폼 및 서비스  \n",
       "300                 스마트 안티 폴루션 추천시스템  \n",
       "301                         스마트 헬스케어  \n",
       "302                            면역억제제  \n",
       "\n",
       "[303 rows x 7 columns]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(second_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(second_result).to_excel('C:/Users/hangy/Desktop/tips_small_predict_result.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_company_pred = mlp_clf.predict(X_tips)\n",
    "y_company_prob = mlp_clf.predict_proba(X_tips)\n",
    "L = np.argsort(-y_company_prob, axis=1)\n",
    "two_pred = L[:,0:3]\n",
    "\n",
    "class_dic = {mlp_clf.classes_[i]: i for i in range(len(mlp_clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dd = []\n",
    "\n",
    "for i in range(len(y_company_prob)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    thrid = two_pred[i][2]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)],key_list[val_list.index(thrid)]])\n",
    "    dd.append({'company':tips_data['company'][i],'origianl_label':tips_data['category'][i],\n",
    "               'predicted_label1':label[0], 'predicted_label2': label[1], 'predicted_label3': label[2]})\n",
    "\n",
    "company_pred_df = pd.DataFrame(dd,columns = ['company','origianl_label','predicted_label1','predicted_label2','predicted_label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "정보통신          121\n",
       "바이오의료          67\n",
       "IT S/W         25\n",
       "기계·소재          23\n",
       "전기전자           22\n",
       "모바일IT          13\n",
       "IT 서비스         10\n",
       "기타              6\n",
       "화학              3\n",
       "기타서비스           3\n",
       "ICT제조           2\n",
       "IoT(사물인터넷)      2\n",
       "IT H/W          2\n",
       "반도체             1\n",
       "정보기기·통신장비       1\n",
       "IT부품            1\n",
       "에너지·자원          1\n",
       "Name: origianl_label, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_pred_df.origianl_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_pred_df.to_excel('C:/Users/hangy/Desktop/tips_predict_result.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calssification for 교수 개발 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "professor=pd.read_csv('C:/Users/hangy/Desktop/professor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "professor_list = []\n",
    "# 텍스트를 가지고 있는 리스트\n",
    "for i in list(professor['content']):\n",
    "    # 숫자 및 특수문자 제거.\n",
    "    t = re.sub('[\\d\\s0-9]',' ',str(i)).strip()\n",
    "    t = re.sub('[=+,#/\\?:^$.@*\\\"※~&%ㆍ·⌬◎◳▢▪!』․\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》→’“”;●•]', ' ', t)\n",
    "    t = re.sub('\\xad', ' ', t)\n",
    "    t = re.sub('  ', ' ', t)\n",
    "    t = re.sub('  ', ' ', t)\n",
    "    t = re.sub('  ', ' ', t)\n",
    "    t = re.sub('및', '', t)\n",
    "    professor_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 28004 from 470 sents. mem=0.990 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=97886, mem=0.996 Gb\n",
      "[Noun Extractor] batch prediction was completed for 10688 words\n",
      "[Noun Extractor] checked compounds. discovered 2949 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 6722 -> 6454\n",
      "[Noun Extractor] postprocessing ignore_features : 6454 -> 6426\n",
      "[Noun Extractor] postprocessing ignore_NJ : 6426 -> 6422\n",
      "[Noun Extractor] 6422 nouns (2949 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.997 Gb                    \n",
      "[Noun Extractor] 73.30 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "professor_corpus=noun_corpus(professor_list)\n",
    "#stopwords 제거\n",
    "stops = list(ko_stopwords['stopwords'])\n",
    "docs=[]\n",
    "for i in range(len(professor_corpus)):\n",
    "    words=[]\n",
    "    for w in professor_corpus[i]:\n",
    "        if (not w in stops) & (len(w)>1):\n",
    "            words.append(w)\n",
    "    docs.append(words)\n",
    "professor['token']=docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ t for d in professor['token'] for t in d]\n",
    "text = nltk.Text(tokens, name='NMSC')\n",
    "fdist = text.vocab()\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['frequency']\n",
    "df_fdist['term'] = list(df_fdist.index)\n",
    "df_fdist = df_fdist.reset_index(drop=True)\n",
    "df_fdist = df_fdist.sort_values([\"frequency\"], ascending=[False])\n",
    "df_fdist = df_fdist.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 개수           :  5800\n",
      "상위 횟수 token 개수 :  10\n",
      "1회 횟수 token 개수  :  4663\n"
     ]
    }
   ],
   "source": [
    "tokens = [ t for d in professor['token'] for t in d]\n",
    "text   = nltk.Text(tokens, name='NMSC')\n",
    "print('token 개수           : ',len(df_fdist))\n",
    "print('상위 횟수 token 개수 : ',len(df_fdist[df_fdist['frequency']>300]))\n",
    "print('1회 횟수 token 개수  : ',len(df_fdist[df_fdist['frequency']<=10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "zif_list = pd.DataFrame(df_fdist[(df_fdist['frequency'] >250) | (df_fdist['frequency'] <= 2)]['term'])\n",
    "zif_list.columns = ['stopwords']\n",
    "zif_list = zif_list.reset_index(drop=True)\n",
    "\n",
    "# zif'slow \n",
    "stops = list(zif_list['stopwords'])\n",
    "def stopwords (stops,corpus):\n",
    "    docs=[]\n",
    "    for i in range(len(corpus)):\n",
    "        words=[]\n",
    "        for w in corpus[i]:\n",
    "            if (not w in stops) & (len(w)>1):\n",
    "                words.append(w)\n",
    "        docs.append(words)\n",
    "    return docs\n",
    "docs = stopwords(stops, professor['token'])\n",
    "professor['token'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = ['사진제공','박사','전남대','사진','제공','저자','전기전자컴퓨터공학부','기자','선준미디어','교재','밝혔다새로','연구사업','공동연구','석사과정','개최','시앙스포','위치','배출','어렵다','교수연구팀','고려대학교','부산대', '제시','GIST','코엑스','네이버','채널','구독','미국','국내','논문','게재','학술지','연구원','뉴시스통신','연구결과','제출','관련','때문','뉴시스','대학원','대통령','프랑스','합격','장관','정무영','이전','주요','인사','자랑','한계','진출','눈길','세계','뉴스','기자','어려움','대학원생','국내외','기관','박사과정','연구진','졸업생','UNIST','광운대학교','총장','있었다','예상된다','대학']\n",
    "docs = stopwords(stops, professor['token'])\n",
    "professor['token'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "professor['class']='None'\n",
    "pro_train_docs = [TaggedDocument(d,c) for d,c in professor[['token', 'class']].values]\n",
    "pro_train = [doc_vectorizer.infer_vector(doc.words) for doc in pro_train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pro_y = mlp_clf.predict(pro_train)\n",
    "\n",
    "pro_y = mlp_clf.predict(pro_train)\n",
    "#print(\"1class 정확도: {:.3f}\".format(accuracy_score(pro_y, pro_train)))\n",
    "pro_y_pred_prob = mlp_clf.predict_proba(pro_train)\n",
    "L = np.argsort(-pro_y_pred_prob, axis=1)\n",
    "two_pred = L[:,0:2]\n",
    "\n",
    "class_dic = {mlp_clf.classes_[i]: i for i in range(len(mlp_clf.classes_))}\n",
    "key_list = list(class_dic.keys()) \n",
    "val_list = list(class_dic.values()) \n",
    "\n",
    "dddd=[]\n",
    "score = []\n",
    "for i in range(len(pro_y)):\n",
    "    first = two_pred[i][0]\n",
    "    second = two_pred[i][1]\n",
    "    label = list([key_list[val_list.index(first)],key_list[val_list.index(second)]])\n",
    "    if y_test[i] in label :\n",
    "        score.append(1)\n",
    "    else : \n",
    "        score.append(0)\n",
    "    dddd.append({'title':professor['title'][i],'y_test':professor['token'][i], 'first':label[0], 'second':label[1]})\n",
    "#acc = sum(score)/len(y_test)\n",
    "#print(\"2class 정확도:\", acc)\n",
    "ddd=pd.DataFrame(dddd)\n",
    "ddd.head(20)\n",
    "professor['predicted_class']='1:'+ddd['first']+' 2:'+ddd['second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>press</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>class</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>광운대 연구진, 다기능 나노 광학 디바이스 개발</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-21 14:21</td>\n",
       "      <td>이상신 사진제공  이상신 광운대학교는 전자공학과 이상신 연구팀이 최덕용 호주국립대 ...</td>\n",
       "      <td>[전자공학, 광학, 디바이스, 렌즈, 광학, 복잡, 구조, 가지, 광학, 소자, 극...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:화학및섬유소재 2:지능형반도체</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이대목동병원 김건하 교수, 챗봇 회사에 ‘치매 예방 훈련용 콘텐츠’ 기술 이전</td>\n",
       "      <td>스포츠서울</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-18 10:26</td>\n",
       "      <td>김건하 교수스포츠서울김건하 이대목동병원 신경과 교수가 챗봇 회사인 주하이와 치매 예...</td>\n",
       "      <td>[신경, 챗봇, 회사, 치매, 예방, 훈련, 콘텐츠, 협약, 체결, 치매, 예방, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:홈어플라이언스 2:의료서비스·기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>남덕우 UNIST 교수팀, 마이크로 RNA 조절 빅데이터 분석시스템 개발</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-17 21:14</td>\n",
       "      <td>남덕우 UNIST 교수팀 왼쪽부터 김진환 연구원 남 윤소라 박사 하이 응우옌 박사U...</td>\n",
       "      <td>[왼쪽, 인간, 마이크로RNA, 조절, 네트워크, 예측, 빅데이터, 분석, 개발했다...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:의료서비스·기기 2:바이오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>박성규 GIST 교수, 혈액 한 방울로 치매 진단 기술 공동 개발</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-17 12:02</td>\n",
       "      <td>국내 연구진이 혈액 한 방울로 치매를 간단히 진단할 수 있는 기술을 개발했다 치매 ...</td>\n",
       "      <td>[혈액, 치매, 간단, 진단, 치매, 조기, 진단, 예방, 기여, 생명과학부, 의생...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:바이오 2:의료서비스·기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>선준브레인센터, 정지향 교수와 의료고문 및 연구 사업진행…선준미디어 교재개발</td>\n",
       "      <td>스포츠조선</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-15 13:06</td>\n",
       "      <td>사진 선준미디어 제공선준브레인센터는 대한신경과학회 치매 대책 특임이사이며 대한치매학...</td>\n",
       "      <td>[치매, 신경, 정지, 의료, 치매안심센터, 치매환자, 참여, 계획, 미디어, 비교...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:스마트미디어기기 2:조선</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>서울백병원 김율리 교수팀, 'IT 기술+의료' 실시간 거식증 치료법 개발</td>\n",
       "      <td>스포츠조선</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-13 14:06</td>\n",
       "      <td>IT 기술과 의료가 접목돼 언제 어디서든 실시간으로 사용 가능한 거식증 치료법이 개...</td>\n",
       "      <td>[IT, 의료, 접목, 어디서, 실시간, 거식증, 치료법, 거식증, 섭식장애, 심각...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:홈어플라이언스 2:의료서비스·기기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'자가진단 스마트빌딩 나온다' 김창석 부산대 교수, 광섬유센서 원천기술 개발</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-13 14:05</td>\n",
       "      <td>실시간 준분포형 광섬유센서 기술을 개발한 김창석 부산대 교수부산대 연구팀이 스스로 ...</td>\n",
       "      <td>[실시간, 광섬유센서, 스스로, 문제, 인지, 진단, 해결, 방안, 스마트, 스마트...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:LED/광 2:정밀기계</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>아주대 서형탁 연구팀, 수소 생성 '고효율 태양광촉매 전극' 개발</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-12 10:04</td>\n",
       "      <td>구리복합산화물기반의‘물분해광전극’개발       아주대 서형탁 교수와 공동연구원 칼...</td>\n",
       "      <td>[구리, 공동연구원, 신소재공학과, 고효율, 전극, 태양광, 분해, 수소, 전극, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:에너지 2:화학및섬유소재</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>포스텍 김원배교수팀, '온난화 주범' 이산화탄소 분해하는 촉매 성공</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-12 3:03</td>\n",
       "      <td>포스텍연구팀이 지구온난화의 주범 이산화탄소를 분해해 산업적 활용가치가 높은 다...</td>\n",
       "      <td>[지구온난화, 주범, 이산화탄소, 분해, 산업적, 활용가치, 물질, 전환, 촉매, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:에너지 2:화학및섬유소재</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>김용진 교수팀, 자궁근종 성장 예측 지표 개발</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>2019-03-11 15:12</td>\n",
       "      <td>김용진 고대구로병원 산부인과 교수고려대학교 구로병원원장 한승규은 김용진 산부인과 교...</td>\n",
       "      <td>[자궁근종, 예후, 예측, 지표, 성장, 양상, 치료, 필요, 경우, 경우, 그동안...</td>\n",
       "      <td>None</td>\n",
       "      <td>1:의료서비스·기기 2:바이오</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title   press  \\\n",
       "0                   광운대 연구진, 다기능 나노 광학 디바이스 개발    매일경제   \n",
       "1  이대목동병원 김건하 교수, 챗봇 회사에 ‘치매 예방 훈련용 콘텐츠’ 기술 이전   스포츠서울   \n",
       "2     남덕우 UNIST 교수팀, 마이크로 RNA 조절 빅데이터 분석시스템 개발    전자신문   \n",
       "3         박성규 GIST 교수, 혈액 한 방울로 치매 진단 기술 공동 개발    전자신문   \n",
       "4   선준브레인센터, 정지향 교수와 의료고문 및 연구 사업진행…선준미디어 교재개발   스포츠조선   \n",
       "5     서울백병원 김율리 교수팀, 'IT 기술+의료' 실시간 거식증 치료법 개발   스포츠조선   \n",
       "6   '자가진단 스마트빌딩 나온다' 김창석 부산대 교수, 광섬유센서 원천기술 개발    전자신문   \n",
       "7         아주대 서형탁 연구팀, 수소 생성 '고효율 태양광촉매 전극' 개발  파이낸셜뉴스   \n",
       "8        포스텍 김원배교수팀, '온난화 주범' 이산화탄소 분해하는 촉매 성공    세계일보   \n",
       "9                    김용진 교수팀, 자궁근종 성장 예측 지표 개발    전자신문   \n",
       "\n",
       "                                                link              date  \\\n",
       "0  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-21 14:21   \n",
       "1  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-18 10:26   \n",
       "2  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-17 21:14   \n",
       "3  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-17 12:02   \n",
       "4  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-15 13:06   \n",
       "5  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-13 14:06   \n",
       "6  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-13 14:05   \n",
       "7  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-12 10:04   \n",
       "8  https://news.naver.com/main/read.nhn?mode=LSD&...   2019-03-12 3:03   \n",
       "9  https://news.naver.com/main/read.nhn?mode=LSD&...  2019-03-11 15:12   \n",
       "\n",
       "                                             content  \\\n",
       "0  이상신 사진제공  이상신 광운대학교는 전자공학과 이상신 연구팀이 최덕용 호주국립대 ...   \n",
       "1  김건하 교수스포츠서울김건하 이대목동병원 신경과 교수가 챗봇 회사인 주하이와 치매 예...   \n",
       "2  남덕우 UNIST 교수팀 왼쪽부터 김진환 연구원 남 윤소라 박사 하이 응우옌 박사U...   \n",
       "3  국내 연구진이 혈액 한 방울로 치매를 간단히 진단할 수 있는 기술을 개발했다 치매 ...   \n",
       "4  사진 선준미디어 제공선준브레인센터는 대한신경과학회 치매 대책 특임이사이며 대한치매학...   \n",
       "5  IT 기술과 의료가 접목돼 언제 어디서든 실시간으로 사용 가능한 거식증 치료법이 개...   \n",
       "6  실시간 준분포형 광섬유센서 기술을 개발한 김창석 부산대 교수부산대 연구팀이 스스로 ...   \n",
       "7  구리복합산화물기반의‘물분해광전극’개발       아주대 서형탁 교수와 공동연구원 칼...   \n",
       "8     포스텍연구팀이 지구온난화의 주범 이산화탄소를 분해해 산업적 활용가치가 높은 다...   \n",
       "9  김용진 고대구로병원 산부인과 교수고려대학교 구로병원원장 한승규은 김용진 산부인과 교...   \n",
       "\n",
       "                                               token class  \\\n",
       "0  [전자공학, 광학, 디바이스, 렌즈, 광학, 복잡, 구조, 가지, 광학, 소자, 극...  None   \n",
       "1  [신경, 챗봇, 회사, 치매, 예방, 훈련, 콘텐츠, 협약, 체결, 치매, 예방, ...  None   \n",
       "2  [왼쪽, 인간, 마이크로RNA, 조절, 네트워크, 예측, 빅데이터, 분석, 개발했다...  None   \n",
       "3  [혈액, 치매, 간단, 진단, 치매, 조기, 진단, 예방, 기여, 생명과학부, 의생...  None   \n",
       "4  [치매, 신경, 정지, 의료, 치매안심센터, 치매환자, 참여, 계획, 미디어, 비교...  None   \n",
       "5  [IT, 의료, 접목, 어디서, 실시간, 거식증, 치료법, 거식증, 섭식장애, 심각...  None   \n",
       "6  [실시간, 광섬유센서, 스스로, 문제, 인지, 진단, 해결, 방안, 스마트, 스마트...  None   \n",
       "7  [구리, 공동연구원, 신소재공학과, 고효율, 전극, 태양광, 분해, 수소, 전극, ...  None   \n",
       "8  [지구온난화, 주범, 이산화탄소, 분해, 산업적, 활용가치, 물질, 전환, 촉매, ...  None   \n",
       "9  [자궁근종, 예후, 예측, 지표, 성장, 양상, 치료, 필요, 경우, 경우, 그동안...  None   \n",
       "\n",
       "        predicted_class  \n",
       "0    1:화학및섬유소재 2:지능형반도체  \n",
       "1  1:홈어플라이언스 2:의료서비스·기기  \n",
       "2      1:의료서비스·기기 2:바이오  \n",
       "3      1:바이오 2:의료서비스·기기  \n",
       "4       1:스마트미디어기기 2:조선  \n",
       "5  1:홈어플라이언스 2:의료서비스·기기  \n",
       "6        1:LED/광 2:정밀기계  \n",
       "7       1:에너지 2:화학및섬유소재  \n",
       "8       1:에너지 2:화학및섬유소재  \n",
       "9      1:의료서비스·기기 2:바이오  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "professor.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "professor.to_excel('C:/Users/hangy/Desktop/professor_predict_result.xlsx',sheet_name='Sheet1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-gpu",
   "language": "python",
   "name": "deeplearning-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
